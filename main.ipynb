{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import sklearn\n",
    "from tensorflow.keras import models, layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "path=r\"train.csv\"\n",
    "df=pd.read_csv(path)\n",
    "if 'UID' in df.columns:\n",
    "    df = df.set_index('UID')\n",
    "    new_df=df.copy()\n",
    "df=df.drop(columns=['ph_no','credit_card_number','email','name','url','cvv','country','job','emoji'])#does not affect the output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y=df['state']\n",
    "df=df.drop(columns=['state'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define a regular expression pattern to extract floating-point numbers\n",
    "pattern = r'(-?\\d+\\.\\d+)'\n",
    "\n",
    "# Use str.extract to apply the pattern column-wise\n",
    "df = df.apply(lambda x: x.str.extract(pattern, expand=False))\n",
    "\n",
    "# Convert the extracted values to numeric\n",
    "df = df.apply(pd.to_numeric, errors='coerce')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>K3Ll9</th>\n",
       "      <th>19rjS</th>\n",
       "      <th>yeIIP</th>\n",
       "      <th>Bw1V5</th>\n",
       "      <th>5k16L</th>\n",
       "      <th>e2l5S</th>\n",
       "      <th>cg31y</th>\n",
       "      <th>8SVMv</th>\n",
       "      <th>Xsi3p</th>\n",
       "      <th>l8Y6n</th>\n",
       "      <th>...</th>\n",
       "      <th>OaMqz</th>\n",
       "      <th>qhUzJ</th>\n",
       "      <th>FpCOT</th>\n",
       "      <th>zEnW3</th>\n",
       "      <th>ASDn5</th>\n",
       "      <th>vF2is</th>\n",
       "      <th>pZijn</th>\n",
       "      <th>WUc3c</th>\n",
       "      <th>sCIyG</th>\n",
       "      <th>qaERi</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>138761.000000</td>\n",
       "      <td>138777.000000</td>\n",
       "      <td>138736.000000</td>\n",
       "      <td>138728.000000</td>\n",
       "      <td>138713.000000</td>\n",
       "      <td>138709.000000</td>\n",
       "      <td>138654.000000</td>\n",
       "      <td>138773.000000</td>\n",
       "      <td>138761.000000</td>\n",
       "      <td>138644.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>138703.000000</td>\n",
       "      <td>138683.000000</td>\n",
       "      <td>138696.000000</td>\n",
       "      <td>138748.000000</td>\n",
       "      <td>138676.000000</td>\n",
       "      <td>138746.000000</td>\n",
       "      <td>138689.000000</td>\n",
       "      <td>138688.000000</td>\n",
       "      <td>138740.000000</td>\n",
       "      <td>138685.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.405623</td>\n",
       "      <td>0.700885</td>\n",
       "      <td>-0.228169</td>\n",
       "      <td>0.038084</td>\n",
       "      <td>0.080741</td>\n",
       "      <td>1.088124</td>\n",
       "      <td>-0.099424</td>\n",
       "      <td>-0.508855</td>\n",
       "      <td>-0.086820</td>\n",
       "      <td>-0.461384</td>\n",
       "      <td>...</td>\n",
       "      <td>1.749606</td>\n",
       "      <td>-0.352595</td>\n",
       "      <td>-0.097552</td>\n",
       "      <td>-0.872785</td>\n",
       "      <td>-0.190779</td>\n",
       "      <td>-0.202942</td>\n",
       "      <td>0.304085</td>\n",
       "      <td>0.021870</td>\n",
       "      <td>0.982778</td>\n",
       "      <td>-0.006658</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>4.207810</td>\n",
       "      <td>17.316074</td>\n",
       "      <td>18.558254</td>\n",
       "      <td>16.804733</td>\n",
       "      <td>4.198671</td>\n",
       "      <td>19.251276</td>\n",
       "      <td>4.144329</td>\n",
       "      <td>17.998698</td>\n",
       "      <td>4.255966</td>\n",
       "      <td>17.002255</td>\n",
       "      <td>...</td>\n",
       "      <td>19.961617</td>\n",
       "      <td>19.173462</td>\n",
       "      <td>4.214975</td>\n",
       "      <td>14.922001</td>\n",
       "      <td>4.232022</td>\n",
       "      <td>4.153763</td>\n",
       "      <td>4.121639</td>\n",
       "      <td>4.232307</td>\n",
       "      <td>19.696209</td>\n",
       "      <td>4.146676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-17.378663</td>\n",
       "      <td>-77.636372</td>\n",
       "      <td>-88.816744</td>\n",
       "      <td>-84.533117</td>\n",
       "      <td>-20.864162</td>\n",
       "      <td>-88.800077</td>\n",
       "      <td>-20.083547</td>\n",
       "      <td>-88.428950</td>\n",
       "      <td>-18.442479</td>\n",
       "      <td>-79.156959</td>\n",
       "      <td>...</td>\n",
       "      <td>-86.398081</td>\n",
       "      <td>-88.274028</td>\n",
       "      <td>-20.444630</td>\n",
       "      <td>-70.024310</td>\n",
       "      <td>-18.823306</td>\n",
       "      <td>-18.528236</td>\n",
       "      <td>-18.311135</td>\n",
       "      <td>-19.833364</td>\n",
       "      <td>-97.657266</td>\n",
       "      <td>-18.691201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>-2.421919</td>\n",
       "      <td>-10.766546</td>\n",
       "      <td>-12.556615</td>\n",
       "      <td>-11.265699</td>\n",
       "      <td>-2.735720</td>\n",
       "      <td>-11.844879</td>\n",
       "      <td>-2.862022</td>\n",
       "      <td>-12.588352</td>\n",
       "      <td>-2.950469</td>\n",
       "      <td>-11.767128</td>\n",
       "      <td>...</td>\n",
       "      <td>-11.667677</td>\n",
       "      <td>-13.188030</td>\n",
       "      <td>-2.926210</td>\n",
       "      <td>-10.804452</td>\n",
       "      <td>-3.040233</td>\n",
       "      <td>-3.016949</td>\n",
       "      <td>-2.471048</td>\n",
       "      <td>-2.826263</td>\n",
       "      <td>-12.231083</td>\n",
       "      <td>-2.800190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.411023</td>\n",
       "      <td>0.829260</td>\n",
       "      <td>-0.215681</td>\n",
       "      <td>-0.115716</td>\n",
       "      <td>0.073549</td>\n",
       "      <td>1.085606</td>\n",
       "      <td>-0.080195</td>\n",
       "      <td>-0.624914</td>\n",
       "      <td>-0.081912</td>\n",
       "      <td>-0.244787</td>\n",
       "      <td>...</td>\n",
       "      <td>1.730703</td>\n",
       "      <td>-0.298878</td>\n",
       "      <td>-0.077281</td>\n",
       "      <td>-0.820220</td>\n",
       "      <td>-0.183061</td>\n",
       "      <td>-0.209118</td>\n",
       "      <td>0.312054</td>\n",
       "      <td>0.007508</td>\n",
       "      <td>0.774899</td>\n",
       "      <td>-0.020755</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>3.224626</td>\n",
       "      <td>12.346383</td>\n",
       "      <td>12.176671</td>\n",
       "      <td>11.143424</td>\n",
       "      <td>2.907034</td>\n",
       "      <td>14.017467</td>\n",
       "      <td>2.689317</td>\n",
       "      <td>11.355298</td>\n",
       "      <td>2.789168</td>\n",
       "      <td>10.971212</td>\n",
       "      <td>...</td>\n",
       "      <td>15.137703</td>\n",
       "      <td>12.558143</td>\n",
       "      <td>2.746927</td>\n",
       "      <td>9.170924</td>\n",
       "      <td>2.670074</td>\n",
       "      <td>2.589425</td>\n",
       "      <td>3.092985</td>\n",
       "      <td>2.865800</td>\n",
       "      <td>14.006714</td>\n",
       "      <td>2.767928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>19.441730</td>\n",
       "      <td>81.743863</td>\n",
       "      <td>77.957198</td>\n",
       "      <td>76.521389</td>\n",
       "      <td>18.870930</td>\n",
       "      <td>86.574856</td>\n",
       "      <td>19.113702</td>\n",
       "      <td>85.249397</td>\n",
       "      <td>18.095460</td>\n",
       "      <td>80.198581</td>\n",
       "      <td>...</td>\n",
       "      <td>90.727512</td>\n",
       "      <td>90.114963</td>\n",
       "      <td>18.997582</td>\n",
       "      <td>64.256785</td>\n",
       "      <td>19.636270</td>\n",
       "      <td>18.474009</td>\n",
       "      <td>19.516091</td>\n",
       "      <td>18.421846</td>\n",
       "      <td>102.783205</td>\n",
       "      <td>18.189089</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows Ã— 100 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               K3Ll9          19rjS          yeIIP          Bw1V5  \\\n",
       "count  138761.000000  138777.000000  138736.000000  138728.000000   \n",
       "mean        0.405623       0.700885      -0.228169       0.038084   \n",
       "std         4.207810      17.316074      18.558254      16.804733   \n",
       "min       -17.378663     -77.636372     -88.816744     -84.533117   \n",
       "25%        -2.421919     -10.766546     -12.556615     -11.265699   \n",
       "50%         0.411023       0.829260      -0.215681      -0.115716   \n",
       "75%         3.224626      12.346383      12.176671      11.143424   \n",
       "max        19.441730      81.743863      77.957198      76.521389   \n",
       "\n",
       "               5k16L          e2l5S          cg31y          8SVMv  \\\n",
       "count  138713.000000  138709.000000  138654.000000  138773.000000   \n",
       "mean        0.080741       1.088124      -0.099424      -0.508855   \n",
       "std         4.198671      19.251276       4.144329      17.998698   \n",
       "min       -20.864162     -88.800077     -20.083547     -88.428950   \n",
       "25%        -2.735720     -11.844879      -2.862022     -12.588352   \n",
       "50%         0.073549       1.085606      -0.080195      -0.624914   \n",
       "75%         2.907034      14.017467       2.689317      11.355298   \n",
       "max        18.870930      86.574856      19.113702      85.249397   \n",
       "\n",
       "               Xsi3p          l8Y6n  ...          OaMqz          qhUzJ  \\\n",
       "count  138761.000000  138644.000000  ...  138703.000000  138683.000000   \n",
       "mean       -0.086820      -0.461384  ...       1.749606      -0.352595   \n",
       "std         4.255966      17.002255  ...      19.961617      19.173462   \n",
       "min       -18.442479     -79.156959  ...     -86.398081     -88.274028   \n",
       "25%        -2.950469     -11.767128  ...     -11.667677     -13.188030   \n",
       "50%        -0.081912      -0.244787  ...       1.730703      -0.298878   \n",
       "75%         2.789168      10.971212  ...      15.137703      12.558143   \n",
       "max        18.095460      80.198581  ...      90.727512      90.114963   \n",
       "\n",
       "               FpCOT          zEnW3          ASDn5          vF2is  \\\n",
       "count  138696.000000  138748.000000  138676.000000  138746.000000   \n",
       "mean       -0.097552      -0.872785      -0.190779      -0.202942   \n",
       "std         4.214975      14.922001       4.232022       4.153763   \n",
       "min       -20.444630     -70.024310     -18.823306     -18.528236   \n",
       "25%        -2.926210     -10.804452      -3.040233      -3.016949   \n",
       "50%        -0.077281      -0.820220      -0.183061      -0.209118   \n",
       "75%         2.746927       9.170924       2.670074       2.589425   \n",
       "max        18.997582      64.256785      19.636270      18.474009   \n",
       "\n",
       "               pZijn          WUc3c          sCIyG          qaERi  \n",
       "count  138689.000000  138688.000000  138740.000000  138685.000000  \n",
       "mean        0.304085       0.021870       0.982778      -0.006658  \n",
       "std         4.121639       4.232307      19.696209       4.146676  \n",
       "min       -18.311135     -19.833364     -97.657266     -18.691201  \n",
       "25%        -2.471048      -2.826263     -12.231083      -2.800190  \n",
       "50%         0.312054       0.007508       0.774899      -0.020755  \n",
       "75%         3.092985       2.865800      14.006714       2.767928  \n",
       "max        19.516091      18.421846     102.783205      18.189089  \n",
       "\n",
       "[8 rows x 100 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>K3Ll9</th>\n",
       "      <th>19rjS</th>\n",
       "      <th>yeIIP</th>\n",
       "      <th>Bw1V5</th>\n",
       "      <th>5k16L</th>\n",
       "      <th>e2l5S</th>\n",
       "      <th>cg31y</th>\n",
       "      <th>8SVMv</th>\n",
       "      <th>Xsi3p</th>\n",
       "      <th>l8Y6n</th>\n",
       "      <th>...</th>\n",
       "      <th>OaMqz</th>\n",
       "      <th>qhUzJ</th>\n",
       "      <th>FpCOT</th>\n",
       "      <th>zEnW3</th>\n",
       "      <th>ASDn5</th>\n",
       "      <th>vF2is</th>\n",
       "      <th>pZijn</th>\n",
       "      <th>WUc3c</th>\n",
       "      <th>sCIyG</th>\n",
       "      <th>qaERi</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.271757</td>\n",
       "      <td>0.003041</td>\n",
       "      <td>0.663157</td>\n",
       "      <td>0.774856</td>\n",
       "      <td>0.386445</td>\n",
       "      <td>0.722778</td>\n",
       "      <td>0.409112</td>\n",
       "      <td>0.429566</td>\n",
       "      <td>0.519147</td>\n",
       "      <td>0.178729</td>\n",
       "      <td>...</td>\n",
       "      <td>0.010459</td>\n",
       "      <td>0.692546</td>\n",
       "      <td>0.481381</td>\n",
       "      <td>0.144018</td>\n",
       "      <td>0.093866</td>\n",
       "      <td>0.875010</td>\n",
       "      <td>0.601252</td>\n",
       "      <td>0.294314</td>\n",
       "      <td>0.640884</td>\n",
       "      <td>0.048658</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.583687</td>\n",
       "      <td>0.333642</td>\n",
       "      <td>0.179308</td>\n",
       "      <td>0.921702</td>\n",
       "      <td>0.782962</td>\n",
       "      <td>0.769680</td>\n",
       "      <td>0.676964</td>\n",
       "      <td>0.933893</td>\n",
       "      <td>0.788037</td>\n",
       "      <td>0.208606</td>\n",
       "      <td>...</td>\n",
       "      <td>0.801875</td>\n",
       "      <td>0.019077</td>\n",
       "      <td>0.824425</td>\n",
       "      <td>0.089255</td>\n",
       "      <td>0.864604</td>\n",
       "      <td>0.508792</td>\n",
       "      <td>0.774877</td>\n",
       "      <td>0.979525</td>\n",
       "      <td>0.472472</td>\n",
       "      <td>0.003785</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.840993</td>\n",
       "      <td>0.886129</td>\n",
       "      <td>0.993347</td>\n",
       "      <td>0.126438</td>\n",
       "      <td>0.149543</td>\n",
       "      <td>0.770834</td>\n",
       "      <td>0.719599</td>\n",
       "      <td>0.017780</td>\n",
       "      <td>0.498999</td>\n",
       "      <td>0.994047</td>\n",
       "      <td>...</td>\n",
       "      <td>0.365798</td>\n",
       "      <td>0.961911</td>\n",
       "      <td>0.945858</td>\n",
       "      <td>0.987066</td>\n",
       "      <td>0.405011</td>\n",
       "      <td>0.315921</td>\n",
       "      <td>0.940835</td>\n",
       "      <td>0.658352</td>\n",
       "      <td>0.028621</td>\n",
       "      <td>0.980604</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.060188</td>\n",
       "      <td>0.716435</td>\n",
       "      <td>0.120056</td>\n",
       "      <td>0.987264</td>\n",
       "      <td>0.943357</td>\n",
       "      <td>0.186924</td>\n",
       "      <td>0.593681</td>\n",
       "      <td>0.765008</td>\n",
       "      <td>0.207607</td>\n",
       "      <td>0.035932</td>\n",
       "      <td>...</td>\n",
       "      <td>0.269076</td>\n",
       "      <td>0.273961</td>\n",
       "      <td>0.006481</td>\n",
       "      <td>0.964306</td>\n",
       "      <td>0.707942</td>\n",
       "      <td>0.965832</td>\n",
       "      <td>0.441247</td>\n",
       "      <td>0.095295</td>\n",
       "      <td>0.886203</td>\n",
       "      <td>0.416695</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.038921</td>\n",
       "      <td>0.211960</td>\n",
       "      <td>0.586805</td>\n",
       "      <td>0.646161</td>\n",
       "      <td>0.133786</td>\n",
       "      <td>0.519417</td>\n",
       "      <td>0.068101</td>\n",
       "      <td>0.105524</td>\n",
       "      <td>0.018151</td>\n",
       "      <td>0.081068</td>\n",
       "      <td>...</td>\n",
       "      <td>0.480159</td>\n",
       "      <td>0.723816</td>\n",
       "      <td>0.357199</td>\n",
       "      <td>0.124333</td>\n",
       "      <td>0.531105</td>\n",
       "      <td>0.831907</td>\n",
       "      <td>0.848252</td>\n",
       "      <td>0.181024</td>\n",
       "      <td>0.814587</td>\n",
       "      <td>0.992897</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139995</th>\n",
       "      <td>0.643723</td>\n",
       "      <td>0.336090</td>\n",
       "      <td>0.147399</td>\n",
       "      <td>0.247794</td>\n",
       "      <td>0.812493</td>\n",
       "      <td>0.205767</td>\n",
       "      <td>0.799627</td>\n",
       "      <td>0.830101</td>\n",
       "      <td>0.242611</td>\n",
       "      <td>0.480824</td>\n",
       "      <td>...</td>\n",
       "      <td>0.718091</td>\n",
       "      <td>0.559356</td>\n",
       "      <td>0.620530</td>\n",
       "      <td>0.541286</td>\n",
       "      <td>0.529100</td>\n",
       "      <td>0.888733</td>\n",
       "      <td>0.349982</td>\n",
       "      <td>0.488737</td>\n",
       "      <td>0.905001</td>\n",
       "      <td>0.664178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139996</th>\n",
       "      <td>0.271447</td>\n",
       "      <td>0.540110</td>\n",
       "      <td>0.959523</td>\n",
       "      <td>0.592044</td>\n",
       "      <td>0.793190</td>\n",
       "      <td>0.931877</td>\n",
       "      <td>0.442808</td>\n",
       "      <td>0.171778</td>\n",
       "      <td>0.165375</td>\n",
       "      <td>0.207167</td>\n",
       "      <td>...</td>\n",
       "      <td>0.047619</td>\n",
       "      <td>0.280916</td>\n",
       "      <td>0.670814</td>\n",
       "      <td>0.429358</td>\n",
       "      <td>0.972819</td>\n",
       "      <td>0.693966</td>\n",
       "      <td>0.919222</td>\n",
       "      <td>0.582420</td>\n",
       "      <td>0.929762</td>\n",
       "      <td>0.889338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139997</th>\n",
       "      <td>0.416964</td>\n",
       "      <td>0.319759</td>\n",
       "      <td>0.999001</td>\n",
       "      <td>0.722396</td>\n",
       "      <td>0.277019</td>\n",
       "      <td>0.520766</td>\n",
       "      <td>0.100374</td>\n",
       "      <td>0.436558</td>\n",
       "      <td>0.054103</td>\n",
       "      <td>0.997397</td>\n",
       "      <td>...</td>\n",
       "      <td>0.645675</td>\n",
       "      <td>0.474312</td>\n",
       "      <td>0.324348</td>\n",
       "      <td>0.646453</td>\n",
       "      <td>0.219451</td>\n",
       "      <td>0.623757</td>\n",
       "      <td>0.215796</td>\n",
       "      <td>0.566319</td>\n",
       "      <td>0.790054</td>\n",
       "      <td>0.966279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139998</th>\n",
       "      <td>0.487548</td>\n",
       "      <td>0.519642</td>\n",
       "      <td>0.652498</td>\n",
       "      <td>0.789285</td>\n",
       "      <td>0.580805</td>\n",
       "      <td>0.943057</td>\n",
       "      <td>0.030974</td>\n",
       "      <td>0.747444</td>\n",
       "      <td>0.070794</td>\n",
       "      <td>0.802853</td>\n",
       "      <td>...</td>\n",
       "      <td>0.380967</td>\n",
       "      <td>0.480800</td>\n",
       "      <td>0.413228</td>\n",
       "      <td>0.603914</td>\n",
       "      <td>0.819815</td>\n",
       "      <td>0.810227</td>\n",
       "      <td>0.116801</td>\n",
       "      <td>0.229466</td>\n",
       "      <td>0.742291</td>\n",
       "      <td>0.085354</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139999</th>\n",
       "      <td>0.306567</td>\n",
       "      <td>0.705860</td>\n",
       "      <td>0.616514</td>\n",
       "      <td>0.860750</td>\n",
       "      <td>0.568722</td>\n",
       "      <td>0.827014</td>\n",
       "      <td>0.031602</td>\n",
       "      <td>0.193299</td>\n",
       "      <td>0.623618</td>\n",
       "      <td>0.187460</td>\n",
       "      <td>...</td>\n",
       "      <td>0.137111</td>\n",
       "      <td>0.310692</td>\n",
       "      <td>0.734760</td>\n",
       "      <td>0.019198</td>\n",
       "      <td>0.302057</td>\n",
       "      <td>0.257638</td>\n",
       "      <td>0.795042</td>\n",
       "      <td>0.385158</td>\n",
       "      <td>0.264679</td>\n",
       "      <td>0.976740</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>140000 rows Ã— 100 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           K3Ll9     19rjS     yeIIP     Bw1V5     5k16L     e2l5S     cg31y  \\\n",
       "0       0.271757  0.003041  0.663157  0.774856  0.386445  0.722778  0.409112   \n",
       "1       0.583687  0.333642  0.179308  0.921702  0.782962  0.769680  0.676964   \n",
       "2       0.840993  0.886129  0.993347  0.126438  0.149543  0.770834  0.719599   \n",
       "3       0.060188  0.716435  0.120056  0.987264  0.943357  0.186924  0.593681   \n",
       "4       0.038921  0.211960  0.586805  0.646161  0.133786  0.519417  0.068101   \n",
       "...          ...       ...       ...       ...       ...       ...       ...   \n",
       "139995  0.643723  0.336090  0.147399  0.247794  0.812493  0.205767  0.799627   \n",
       "139996  0.271447  0.540110  0.959523  0.592044  0.793190  0.931877  0.442808   \n",
       "139997  0.416964  0.319759  0.999001  0.722396  0.277019  0.520766  0.100374   \n",
       "139998  0.487548  0.519642  0.652498  0.789285  0.580805  0.943057  0.030974   \n",
       "139999  0.306567  0.705860  0.616514  0.860750  0.568722  0.827014  0.031602   \n",
       "\n",
       "           8SVMv     Xsi3p     l8Y6n  ...     OaMqz     qhUzJ     FpCOT  \\\n",
       "0       0.429566  0.519147  0.178729  ...  0.010459  0.692546  0.481381   \n",
       "1       0.933893  0.788037  0.208606  ...  0.801875  0.019077  0.824425   \n",
       "2       0.017780  0.498999  0.994047  ...  0.365798  0.961911  0.945858   \n",
       "3       0.765008  0.207607  0.035932  ...  0.269076  0.273961  0.006481   \n",
       "4       0.105524  0.018151  0.081068  ...  0.480159  0.723816  0.357199   \n",
       "...          ...       ...       ...  ...       ...       ...       ...   \n",
       "139995  0.830101  0.242611  0.480824  ...  0.718091  0.559356  0.620530   \n",
       "139996  0.171778  0.165375  0.207167  ...  0.047619  0.280916  0.670814   \n",
       "139997  0.436558  0.054103  0.997397  ...  0.645675  0.474312  0.324348   \n",
       "139998  0.747444  0.070794  0.802853  ...  0.380967  0.480800  0.413228   \n",
       "139999  0.193299  0.623618  0.187460  ...  0.137111  0.310692  0.734760   \n",
       "\n",
       "           zEnW3     ASDn5     vF2is     pZijn     WUc3c     sCIyG     qaERi  \n",
       "0       0.144018  0.093866  0.875010  0.601252  0.294314  0.640884  0.048658  \n",
       "1       0.089255  0.864604  0.508792  0.774877  0.979525  0.472472  0.003785  \n",
       "2       0.987066  0.405011  0.315921  0.940835  0.658352  0.028621  0.980604  \n",
       "3       0.964306  0.707942  0.965832  0.441247  0.095295  0.886203  0.416695  \n",
       "4       0.124333  0.531105  0.831907  0.848252  0.181024  0.814587  0.992897  \n",
       "...          ...       ...       ...       ...       ...       ...       ...  \n",
       "139995  0.541286  0.529100  0.888733  0.349982  0.488737  0.905001  0.664178  \n",
       "139996  0.429358  0.972819  0.693966  0.919222  0.582420  0.929762  0.889338  \n",
       "139997  0.646453  0.219451  0.623757  0.215796  0.566319  0.790054  0.966279  \n",
       "139998  0.603914  0.819815  0.810227  0.116801  0.229466  0.742291  0.085354  \n",
       "139999  0.019198  0.302057  0.257638  0.795042  0.385158  0.264679  0.976740  \n",
       "\n",
       "[140000 rows x 100 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# df_imputed = df.fillna(df.median())\n",
    "#use knn\n",
    "# from sklearn.impute import KNNImputer\n",
    "# imputer = KNNImputer(n_neighbors=2)\n",
    "# df_imputed = imputer.fit_transform(df)\n",
    "# df_imputed = pd.DataFrame(df_imputed, columns = df.columns)\n",
    "# df_imputed.describe()\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "imputer = SimpleImputer(strategy='median')\n",
    "imputed_data = imputer.fit_transform(df)\n",
    "\n",
    "from sklearn.preprocessing import QuantileTransformer\n",
    "\n",
    "\n",
    "transformer = QuantileTransformer(n_quantiles=1000, output_distribution='uniform')\n",
    "\n",
    "# Fit and transform the entire dataset\n",
    "transformed_data = transformer.fit_transform(imputed_data)\n",
    "\n",
    "# Convert the transformed data back to a DataFrame\n",
    "df_imputed = pd.DataFrame(transformed_data, columns=df.columns)\n",
    "\n",
    "# Display the transformed DataFrame\n",
    "df_imputed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: >"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.heatmap(df_imputed.isnull(),yticklabels=False,cbar=False,cmap='viridis')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#handle correlated features\n",
    "corr_matrix = df_imputed.corr().abs()\n",
    "upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape, dtype=bool), k=1))\n",
    "to_drop = [column for column in upper.columns if any(upper[column] > 0.95)]\n",
    "df_imputed=df_imputed.drop(columns=to_drop)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['7hdd4', 'aUxm5', 'eWCdk', 'sRaqu', 'op6uG', 'dTj0P', 'pZijn', 'WUc3c']\n"
     ]
    }
   ],
   "source": [
    "print(to_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def outliers_to_median(df):\n",
    "    for col in df.columns:\n",
    "        median = df[col].median()\n",
    "        q1 = df[col].quantile(0.25)\n",
    "        q3 = df[col].quantile(0.75)\n",
    "        iqr = q3 - q1\n",
    "        df[col] = df[col].apply(lambda x: median if x < (q1 - 1.5 * iqr) or x > (q3 + 1.5 * iqr) else x)\n",
    "    return df\n",
    "\n",
    "# Apply outlier handling to the entire DataFrame\n",
    "df = outliers_to_median(df_imputed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler=MinMaxScaler()\n",
    "scaled_data=scaler.fit_transform(df)\n",
    "scaled_df = pd.DataFrame(scaled_data, columns=df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>K3Ll9</th>\n",
       "      <th>19rjS</th>\n",
       "      <th>yeIIP</th>\n",
       "      <th>Bw1V5</th>\n",
       "      <th>5k16L</th>\n",
       "      <th>e2l5S</th>\n",
       "      <th>cg31y</th>\n",
       "      <th>8SVMv</th>\n",
       "      <th>Xsi3p</th>\n",
       "      <th>l8Y6n</th>\n",
       "      <th>...</th>\n",
       "      <th>Bz7Ov</th>\n",
       "      <th>uN0aA</th>\n",
       "      <th>OaMqz</th>\n",
       "      <th>qhUzJ</th>\n",
       "      <th>FpCOT</th>\n",
       "      <th>zEnW3</th>\n",
       "      <th>ASDn5</th>\n",
       "      <th>vF2is</th>\n",
       "      <th>sCIyG</th>\n",
       "      <th>qaERi</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>140000.000000</td>\n",
       "      <td>140000.000000</td>\n",
       "      <td>140000.000000</td>\n",
       "      <td>140000.000000</td>\n",
       "      <td>140000.000000</td>\n",
       "      <td>140000.000000</td>\n",
       "      <td>140000.000000</td>\n",
       "      <td>140000.000000</td>\n",
       "      <td>140000.000000</td>\n",
       "      <td>140000.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>140000.000000</td>\n",
       "      <td>140000.000000</td>\n",
       "      <td>140000.000000</td>\n",
       "      <td>140000.000000</td>\n",
       "      <td>140000.000000</td>\n",
       "      <td>140000.000000</td>\n",
       "      <td>140000.000000</td>\n",
       "      <td>140000.000000</td>\n",
       "      <td>140000.000000</td>\n",
       "      <td>140000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.502261</td>\n",
       "      <td>0.496500</td>\n",
       "      <td>0.501641</td>\n",
       "      <td>0.499231</td>\n",
       "      <td>0.499553</td>\n",
       "      <td>0.497397</td>\n",
       "      <td>0.497015</td>\n",
       "      <td>0.501499</td>\n",
       "      <td>0.497171</td>\n",
       "      <td>0.500590</td>\n",
       "      <td>...</td>\n",
       "      <td>0.494829</td>\n",
       "      <td>0.500513</td>\n",
       "      <td>0.501342</td>\n",
       "      <td>0.501476</td>\n",
       "      <td>0.496855</td>\n",
       "      <td>0.496580</td>\n",
       "      <td>0.498374</td>\n",
       "      <td>0.500768</td>\n",
       "      <td>0.502470</td>\n",
       "      <td>0.501279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.288928</td>\n",
       "      <td>0.288267</td>\n",
       "      <td>0.287812</td>\n",
       "      <td>0.290360</td>\n",
       "      <td>0.288604</td>\n",
       "      <td>0.289072</td>\n",
       "      <td>0.289263</td>\n",
       "      <td>0.287647</td>\n",
       "      <td>0.288736</td>\n",
       "      <td>0.289462</td>\n",
       "      <td>...</td>\n",
       "      <td>0.289312</td>\n",
       "      <td>0.289448</td>\n",
       "      <td>0.287953</td>\n",
       "      <td>0.287410</td>\n",
       "      <td>0.288237</td>\n",
       "      <td>0.289311</td>\n",
       "      <td>0.289960</td>\n",
       "      <td>0.290156</td>\n",
       "      <td>0.287081</td>\n",
       "      <td>0.287581</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.251236</td>\n",
       "      <td>0.247276</td>\n",
       "      <td>0.252738</td>\n",
       "      <td>0.246124</td>\n",
       "      <td>0.250375</td>\n",
       "      <td>0.245968</td>\n",
       "      <td>0.244436</td>\n",
       "      <td>0.250560</td>\n",
       "      <td>0.245286</td>\n",
       "      <td>0.251778</td>\n",
       "      <td>...</td>\n",
       "      <td>0.243819</td>\n",
       "      <td>0.251548</td>\n",
       "      <td>0.254648</td>\n",
       "      <td>0.253307</td>\n",
       "      <td>0.243458</td>\n",
       "      <td>0.244474</td>\n",
       "      <td>0.245537</td>\n",
       "      <td>0.247927</td>\n",
       "      <td>0.258340</td>\n",
       "      <td>0.252775</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.504004</td>\n",
       "      <td>0.494494</td>\n",
       "      <td>0.506006</td>\n",
       "      <td>0.495996</td>\n",
       "      <td>0.494494</td>\n",
       "      <td>0.495996</td>\n",
       "      <td>0.495495</td>\n",
       "      <td>0.503504</td>\n",
       "      <td>0.498999</td>\n",
       "      <td>0.501001</td>\n",
       "      <td>...</td>\n",
       "      <td>0.492993</td>\n",
       "      <td>0.501502</td>\n",
       "      <td>0.502503</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.494995</td>\n",
       "      <td>0.496496</td>\n",
       "      <td>0.496496</td>\n",
       "      <td>0.502503</td>\n",
       "      <td>0.503003</td>\n",
       "      <td>0.498498</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.752941</td>\n",
       "      <td>0.741887</td>\n",
       "      <td>0.750001</td>\n",
       "      <td>0.752388</td>\n",
       "      <td>0.749052</td>\n",
       "      <td>0.750250</td>\n",
       "      <td>0.748908</td>\n",
       "      <td>0.750551</td>\n",
       "      <td>0.745620</td>\n",
       "      <td>0.753402</td>\n",
       "      <td>...</td>\n",
       "      <td>0.745935</td>\n",
       "      <td>0.749893</td>\n",
       "      <td>0.749626</td>\n",
       "      <td>0.747778</td>\n",
       "      <td>0.746896</td>\n",
       "      <td>0.749931</td>\n",
       "      <td>0.749344</td>\n",
       "      <td>0.750759</td>\n",
       "      <td>0.747740</td>\n",
       "      <td>0.751357</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows Ã— 92 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               K3Ll9          19rjS          yeIIP          Bw1V5  \\\n",
       "count  140000.000000  140000.000000  140000.000000  140000.000000   \n",
       "mean        0.502261       0.496500       0.501641       0.499231   \n",
       "std         0.288928       0.288267       0.287812       0.290360   \n",
       "min         0.000000       0.000000       0.000000       0.000000   \n",
       "25%         0.251236       0.247276       0.252738       0.246124   \n",
       "50%         0.504004       0.494494       0.506006       0.495996   \n",
       "75%         0.752941       0.741887       0.750001       0.752388   \n",
       "max         1.000000       1.000000       1.000000       1.000000   \n",
       "\n",
       "               5k16L          e2l5S          cg31y          8SVMv  \\\n",
       "count  140000.000000  140000.000000  140000.000000  140000.000000   \n",
       "mean        0.499553       0.497397       0.497015       0.501499   \n",
       "std         0.288604       0.289072       0.289263       0.287647   \n",
       "min         0.000000       0.000000       0.000000       0.000000   \n",
       "25%         0.250375       0.245968       0.244436       0.250560   \n",
       "50%         0.494494       0.495996       0.495495       0.503504   \n",
       "75%         0.749052       0.750250       0.748908       0.750551   \n",
       "max         1.000000       1.000000       1.000000       1.000000   \n",
       "\n",
       "               Xsi3p          l8Y6n  ...          Bz7Ov          uN0aA  \\\n",
       "count  140000.000000  140000.000000  ...  140000.000000  140000.000000   \n",
       "mean        0.497171       0.500590  ...       0.494829       0.500513   \n",
       "std         0.288736       0.289462  ...       0.289312       0.289448   \n",
       "min         0.000000       0.000000  ...       0.000000       0.000000   \n",
       "25%         0.245286       0.251778  ...       0.243819       0.251548   \n",
       "50%         0.498999       0.501001  ...       0.492993       0.501502   \n",
       "75%         0.745620       0.753402  ...       0.745935       0.749893   \n",
       "max         1.000000       1.000000  ...       1.000000       1.000000   \n",
       "\n",
       "               OaMqz          qhUzJ          FpCOT          zEnW3  \\\n",
       "count  140000.000000  140000.000000  140000.000000  140000.000000   \n",
       "mean        0.501342       0.501476       0.496855       0.496580   \n",
       "std         0.287953       0.287410       0.288237       0.289311   \n",
       "min         0.000000       0.000000       0.000000       0.000000   \n",
       "25%         0.254648       0.253307       0.243458       0.244474   \n",
       "50%         0.502503       0.500000       0.494995       0.496496   \n",
       "75%         0.749626       0.747778       0.746896       0.749931   \n",
       "max         1.000000       1.000000       1.000000       1.000000   \n",
       "\n",
       "               ASDn5          vF2is          sCIyG          qaERi  \n",
       "count  140000.000000  140000.000000  140000.000000  140000.000000  \n",
       "mean        0.498374       0.500768       0.502470       0.501279  \n",
       "std         0.289960       0.290156       0.287081       0.287581  \n",
       "min         0.000000       0.000000       0.000000       0.000000  \n",
       "25%         0.245537       0.247927       0.258340       0.252775  \n",
       "50%         0.496496       0.502503       0.503003       0.498498  \n",
       "75%         0.749344       0.750759       0.747740       0.751357  \n",
       "max         1.000000       1.000000       1.000000       1.000000  \n",
       "\n",
       "[8 rows x 92 columns]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaled_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 1., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 1.],\n",
       "       [0., 0., 0., ..., 1., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 1., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 1.]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "x=scaled_df\n",
    "#one hot encoded y\n",
    "enc=OneHotEncoder()\n",
    "y=enc.fit_transform(np.array(Y).reshape(-1,1)).toarray()\n",
    "\n",
    "x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.2,random_state=42)\n",
    "x_train.shape,x_test.shape,y_train.shape,y_test.shape\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.Sequential()\n",
    "model.add(layers.Dense(500, activation='relu',input_shape=(x_train.shape[1],)))\n",
    "model.add(layers.Dense(100, activation='relu'))\n",
    "model.add(layers.Dense(16, activation='relu'))\n",
    "model.add(layers.Dense(10, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=tf.keras.optimizers.SGD(),loss='categorical_crossentropy',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/55\n",
      "1120/1120 [==============================] - 7s 6ms/step - loss: 0.9550 - accuracy: 0.6940 - val_loss: 0.8897 - val_accuracy: 0.7165\n",
      "Epoch 2/55\n",
      "1120/1120 [==============================] - 6s 5ms/step - loss: 0.7820 - accuracy: 0.7569 - val_loss: 0.7665 - val_accuracy: 0.7551\n",
      "Epoch 3/55\n",
      "1120/1120 [==============================] - 6s 5ms/step - loss: 0.6452 - accuracy: 0.8037 - val_loss: 0.6225 - val_accuracy: 0.8090\n",
      "Epoch 4/55\n",
      "1120/1120 [==============================] - 6s 5ms/step - loss: 0.5426 - accuracy: 0.8374 - val_loss: 0.5686 - val_accuracy: 0.8231\n",
      "Epoch 5/55\n",
      "1120/1120 [==============================] - 5s 5ms/step - loss: 0.4704 - accuracy: 0.8607 - val_loss: 0.4721 - val_accuracy: 0.8610\n",
      "Epoch 6/55\n",
      "1120/1120 [==============================] - 4s 4ms/step - loss: 0.4145 - accuracy: 0.8796 - val_loss: 0.4389 - val_accuracy: 0.8675\n",
      "Epoch 7/55\n",
      "1120/1120 [==============================] - 4s 4ms/step - loss: 0.3714 - accuracy: 0.8954 - val_loss: 0.4137 - val_accuracy: 0.8769\n",
      "Epoch 8/55\n",
      "1120/1120 [==============================] - 6s 5ms/step - loss: 0.3377 - accuracy: 0.9068 - val_loss: 0.3838 - val_accuracy: 0.8874\n",
      "Epoch 9/55\n",
      "1120/1120 [==============================] - 6s 5ms/step - loss: 0.3115 - accuracy: 0.9152 - val_loss: 0.3557 - val_accuracy: 0.8979\n",
      "Epoch 10/55\n",
      "1120/1120 [==============================] - 6s 5ms/step - loss: 0.2908 - accuracy: 0.9224 - val_loss: 0.3385 - val_accuracy: 0.9022\n",
      "Epoch 11/55\n",
      "1120/1120 [==============================] - 5s 4ms/step - loss: 0.2732 - accuracy: 0.9286 - val_loss: 0.3383 - val_accuracy: 0.9031\n",
      "Epoch 12/55\n",
      "1120/1120 [==============================] - 4s 4ms/step - loss: 0.2572 - accuracy: 0.9343 - val_loss: 0.2958 - val_accuracy: 0.9193\n",
      "Epoch 13/55\n",
      "1120/1120 [==============================] - 4s 4ms/step - loss: 0.2450 - accuracy: 0.9379 - val_loss: 0.2929 - val_accuracy: 0.9191\n",
      "Epoch 14/55\n",
      "1120/1120 [==============================] - 4s 4ms/step - loss: 0.2345 - accuracy: 0.9411 - val_loss: 0.3090 - val_accuracy: 0.9140\n",
      "Epoch 15/55\n",
      "1120/1120 [==============================] - 4s 4ms/step - loss: 0.2240 - accuracy: 0.9457 - val_loss: 0.2810 - val_accuracy: 0.9222\n",
      "Epoch 16/55\n",
      "1120/1120 [==============================] - 5s 4ms/step - loss: 0.2147 - accuracy: 0.9485 - val_loss: 0.2744 - val_accuracy: 0.9258\n",
      "Epoch 17/55\n",
      "1120/1120 [==============================] - 5s 5ms/step - loss: 0.2078 - accuracy: 0.9506 - val_loss: 0.2715 - val_accuracy: 0.9275\n",
      "Epoch 18/55\n",
      "1120/1120 [==============================] - 4s 4ms/step - loss: 0.2017 - accuracy: 0.9531 - val_loss: 0.2624 - val_accuracy: 0.9312\n",
      "Epoch 19/55\n",
      "1120/1120 [==============================] - 4s 4ms/step - loss: 0.1951 - accuracy: 0.9550 - val_loss: 0.2650 - val_accuracy: 0.9288\n",
      "Epoch 20/55\n",
      "1120/1120 [==============================] - 5s 5ms/step - loss: 0.1903 - accuracy: 0.9561 - val_loss: 0.2572 - val_accuracy: 0.9327\n",
      "Epoch 21/55\n",
      "1120/1120 [==============================] - 6s 5ms/step - loss: 0.1846 - accuracy: 0.9586 - val_loss: 0.2515 - val_accuracy: 0.9355\n",
      "Epoch 22/55\n",
      "1120/1120 [==============================] - 5s 4ms/step - loss: 0.1800 - accuracy: 0.9606 - val_loss: 0.2666 - val_accuracy: 0.9287\n",
      "Epoch 23/55\n",
      "1120/1120 [==============================] - 4s 4ms/step - loss: 0.1755 - accuracy: 0.9620 - val_loss: 0.2605 - val_accuracy: 0.9322\n",
      "Epoch 24/55\n",
      "1120/1120 [==============================] - 4s 4ms/step - loss: 0.1717 - accuracy: 0.9625 - val_loss: 0.2478 - val_accuracy: 0.9381\n",
      "Epoch 25/55\n",
      "1120/1120 [==============================] - 4s 4ms/step - loss: 0.1679 - accuracy: 0.9640 - val_loss: 0.2428 - val_accuracy: 0.9401\n",
      "Epoch 26/55\n",
      "1120/1120 [==============================] - 6s 5ms/step - loss: 0.1650 - accuracy: 0.9652 - val_loss: 0.2610 - val_accuracy: 0.9329\n",
      "Epoch 27/55\n",
      "1120/1120 [==============================] - 5s 5ms/step - loss: 0.1609 - accuracy: 0.9666 - val_loss: 0.2470 - val_accuracy: 0.9371\n",
      "Epoch 28/55\n",
      "1120/1120 [==============================] - 5s 4ms/step - loss: 0.1587 - accuracy: 0.9669 - val_loss: 0.2380 - val_accuracy: 0.9414\n",
      "Epoch 29/55\n",
      "1120/1120 [==============================] - 4s 4ms/step - loss: 0.1554 - accuracy: 0.9683 - val_loss: 0.2379 - val_accuracy: 0.9416\n",
      "Epoch 30/55\n",
      "1120/1120 [==============================] - 4s 4ms/step - loss: 0.1527 - accuracy: 0.9690 - val_loss: 0.2345 - val_accuracy: 0.9421\n",
      "Epoch 31/55\n",
      "1120/1120 [==============================] - 4s 4ms/step - loss: 0.1499 - accuracy: 0.9701 - val_loss: 0.2438 - val_accuracy: 0.9396\n",
      "Epoch 32/55\n",
      "1120/1120 [==============================] - 5s 4ms/step - loss: 0.1476 - accuracy: 0.9700 - val_loss: 0.2571 - val_accuracy: 0.9345\n",
      "Epoch 33/55\n",
      "1120/1120 [==============================] - 5s 4ms/step - loss: 0.1459 - accuracy: 0.9711 - val_loss: 0.2395 - val_accuracy: 0.9407\n",
      "Epoch 34/55\n",
      "1120/1120 [==============================] - 4s 4ms/step - loss: 0.1428 - accuracy: 0.9714 - val_loss: 0.2351 - val_accuracy: 0.9428\n",
      "Epoch 35/55\n",
      "1120/1120 [==============================] - 5s 5ms/step - loss: 0.1407 - accuracy: 0.9721 - val_loss: 0.2363 - val_accuracy: 0.9424\n",
      "Epoch 36/55\n",
      "1120/1120 [==============================] - 6s 5ms/step - loss: 0.1382 - accuracy: 0.9727 - val_loss: 0.2424 - val_accuracy: 0.9389\n",
      "Epoch 37/55\n",
      "1120/1120 [==============================] - 6s 5ms/step - loss: 0.1364 - accuracy: 0.9732 - val_loss: 0.2367 - val_accuracy: 0.9430\n",
      "Epoch 38/55\n",
      "1120/1120 [==============================] - 5s 4ms/step - loss: 0.1344 - accuracy: 0.9739 - val_loss: 0.2507 - val_accuracy: 0.9370\n",
      "Epoch 39/55\n",
      "1120/1120 [==============================] - 4s 4ms/step - loss: 0.1330 - accuracy: 0.9744 - val_loss: 0.2367 - val_accuracy: 0.9420\n",
      "Epoch 40/55\n",
      "1120/1120 [==============================] - 4s 4ms/step - loss: 0.1310 - accuracy: 0.9751 - val_loss: 0.2428 - val_accuracy: 0.9404\n",
      "Epoch 41/55\n",
      "1120/1120 [==============================] - 5s 4ms/step - loss: 0.1293 - accuracy: 0.9752 - val_loss: 0.2559 - val_accuracy: 0.9359\n",
      "Epoch 42/55\n",
      "1120/1120 [==============================] - 4s 4ms/step - loss: 0.1276 - accuracy: 0.9750 - val_loss: 0.2471 - val_accuracy: 0.9391\n",
      "Epoch 43/55\n",
      "1120/1120 [==============================] - 5s 4ms/step - loss: 0.1260 - accuracy: 0.9759 - val_loss: 0.2365 - val_accuracy: 0.9416\n",
      "Epoch 44/55\n",
      "1120/1120 [==============================] - 5s 4ms/step - loss: 0.1244 - accuracy: 0.9762 - val_loss: 0.2444 - val_accuracy: 0.9399\n",
      "Epoch 45/55\n",
      "1120/1120 [==============================] - 5s 4ms/step - loss: 0.1225 - accuracy: 0.9770 - val_loss: 0.2412 - val_accuracy: 0.9415\n",
      "Epoch 46/55\n",
      "1120/1120 [==============================] - 5s 4ms/step - loss: 0.1207 - accuracy: 0.9772 - val_loss: 0.2391 - val_accuracy: 0.9433\n",
      "Epoch 47/55\n",
      "1120/1120 [==============================] - 4s 4ms/step - loss: 0.1191 - accuracy: 0.9777 - val_loss: 0.2615 - val_accuracy: 0.9344\n",
      "Epoch 48/55\n",
      "1120/1120 [==============================] - 4s 4ms/step - loss: 0.1186 - accuracy: 0.9773 - val_loss: 0.2370 - val_accuracy: 0.9435\n",
      "Epoch 49/55\n",
      "1120/1120 [==============================] - 5s 4ms/step - loss: 0.1172 - accuracy: 0.9776 - val_loss: 0.2436 - val_accuracy: 0.9416\n",
      "Epoch 50/55\n",
      "1120/1120 [==============================] - 5s 4ms/step - loss: 0.1154 - accuracy: 0.9784 - val_loss: 0.2453 - val_accuracy: 0.9405\n",
      "Epoch 51/55\n",
      "1120/1120 [==============================] - 6s 5ms/step - loss: 0.1140 - accuracy: 0.9787 - val_loss: 0.2543 - val_accuracy: 0.9380\n",
      "Epoch 52/55\n",
      "1120/1120 [==============================] - 5s 5ms/step - loss: 0.1130 - accuracy: 0.9789 - val_loss: 0.2406 - val_accuracy: 0.9427\n",
      "Epoch 53/55\n",
      "1120/1120 [==============================] - 4s 4ms/step - loss: 0.1122 - accuracy: 0.9786 - val_loss: 0.2454 - val_accuracy: 0.9409\n",
      "Epoch 54/55\n",
      "1120/1120 [==============================] - 4s 4ms/step - loss: 0.1102 - accuracy: 0.9793 - val_loss: 0.2642 - val_accuracy: 0.9347\n",
      "Epoch 55/55\n",
      "1120/1120 [==============================] - 4s 4ms/step - loss: 0.1087 - accuracy: 0.9797 - val_loss: 0.2496 - val_accuracy: 0.9400\n"
     ]
    }
   ],
   "source": [
    "trained_model=model.fit(x_train,y_train,batch_size=100,epochs=55,validation_data=(x_test,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "875/875 [==============================] - 3s 3ms/step - loss: 0.2496 - accuracy: 0.9400\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.24962879717350006, 0.9399999976158142]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(x_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 done\n",
      "0.012107142857142858\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32md:\\gdsc\\mic\\main.ipynb Cell 20\u001b[0m line \u001b[0;36m7\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/gdsc/mic/main.ipynb#X20sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m2\u001b[39m):\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/gdsc/mic/main.ipynb#X20sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     rf_model \u001b[39m=\u001b[39m RandomForestClassifier(n_estimators\u001b[39m=\u001b[39m\u001b[39m90\u001b[39m)  \n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/gdsc/mic/main.ipynb#X20sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m     rf_model\u001b[39m.\u001b[39;49mfit(x_train, y_train)\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/gdsc/mic/main.ipynb#X20sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m     ensemble\u001b[39m.\u001b[39mappend(rf_model)\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/gdsc/mic/main.ipynb#X20sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m     \u001b[39mprint\u001b[39m(i,\u001b[39m\"\u001b[39m\u001b[39mdone\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\soham\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\base.py:1151\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1144\u001b[0m     estimator\u001b[39m.\u001b[39m_validate_params()\n\u001b[0;32m   1146\u001b[0m \u001b[39mwith\u001b[39;00m config_context(\n\u001b[0;32m   1147\u001b[0m     skip_parameter_validation\u001b[39m=\u001b[39m(\n\u001b[0;32m   1148\u001b[0m         prefer_skip_nested_validation \u001b[39mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1149\u001b[0m     )\n\u001b[0;32m   1150\u001b[0m ):\n\u001b[1;32m-> 1151\u001b[0m     \u001b[39mreturn\u001b[39;00m fit_method(estimator, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\soham\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:456\u001b[0m, in \u001b[0;36mBaseForest.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    445\u001b[0m trees \u001b[39m=\u001b[39m [\n\u001b[0;32m    446\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_make_estimator(append\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, random_state\u001b[39m=\u001b[39mrandom_state)\n\u001b[0;32m    447\u001b[0m     \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(n_more_estimators)\n\u001b[0;32m    448\u001b[0m ]\n\u001b[0;32m    450\u001b[0m \u001b[39m# Parallel loop: we prefer the threading backend as the Cython code\u001b[39;00m\n\u001b[0;32m    451\u001b[0m \u001b[39m# for fitting the trees is internally releasing the Python GIL\u001b[39;00m\n\u001b[0;32m    452\u001b[0m \u001b[39m# making threading more efficient than multiprocessing in\u001b[39;00m\n\u001b[0;32m    453\u001b[0m \u001b[39m# that case. However, for joblib 0.12+ we respect any\u001b[39;00m\n\u001b[0;32m    454\u001b[0m \u001b[39m# parallel_backend contexts set at a higher level,\u001b[39;00m\n\u001b[0;32m    455\u001b[0m \u001b[39m# since correctness does not rely on using threads.\u001b[39;00m\n\u001b[1;32m--> 456\u001b[0m trees \u001b[39m=\u001b[39m Parallel(\n\u001b[0;32m    457\u001b[0m     n_jobs\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mn_jobs,\n\u001b[0;32m    458\u001b[0m     verbose\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mverbose,\n\u001b[0;32m    459\u001b[0m     prefer\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mthreads\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m    460\u001b[0m )(\n\u001b[0;32m    461\u001b[0m     delayed(_parallel_build_trees)(\n\u001b[0;32m    462\u001b[0m         t,\n\u001b[0;32m    463\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbootstrap,\n\u001b[0;32m    464\u001b[0m         X,\n\u001b[0;32m    465\u001b[0m         y,\n\u001b[0;32m    466\u001b[0m         sample_weight,\n\u001b[0;32m    467\u001b[0m         i,\n\u001b[0;32m    468\u001b[0m         \u001b[39mlen\u001b[39;49m(trees),\n\u001b[0;32m    469\u001b[0m         verbose\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mverbose,\n\u001b[0;32m    470\u001b[0m         class_weight\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mclass_weight,\n\u001b[0;32m    471\u001b[0m         n_samples_bootstrap\u001b[39m=\u001b[39;49mn_samples_bootstrap,\n\u001b[0;32m    472\u001b[0m     )\n\u001b[0;32m    473\u001b[0m     \u001b[39mfor\u001b[39;49;00m i, t \u001b[39min\u001b[39;49;00m \u001b[39menumerate\u001b[39;49m(trees)\n\u001b[0;32m    474\u001b[0m )\n\u001b[0;32m    476\u001b[0m \u001b[39m# Collect newly grown trees\u001b[39;00m\n\u001b[0;32m    477\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mestimators_\u001b[39m.\u001b[39mextend(trees)\n",
      "File \u001b[1;32mc:\\Users\\soham\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\parallel.py:65\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m     60\u001b[0m config \u001b[39m=\u001b[39m get_config()\n\u001b[0;32m     61\u001b[0m iterable_with_config \u001b[39m=\u001b[39m (\n\u001b[0;32m     62\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[0;32m     63\u001b[0m     \u001b[39mfor\u001b[39;00m delayed_func, args, kwargs \u001b[39min\u001b[39;00m iterable\n\u001b[0;32m     64\u001b[0m )\n\u001b[1;32m---> 65\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__call__\u001b[39;49m(iterable_with_config)\n",
      "File \u001b[1;32mc:\\Users\\soham\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\joblib\\parallel.py:1088\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1085\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdispatch_one_batch(iterator):\n\u001b[0;32m   1086\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_iterating \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_original_iterator \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m-> 1088\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdispatch_one_batch(iterator):\n\u001b[0;32m   1089\u001b[0m     \u001b[39mpass\u001b[39;00m\n\u001b[0;32m   1091\u001b[0m \u001b[39mif\u001b[39;00m pre_dispatch \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mall\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mor\u001b[39;00m n_jobs \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m   1092\u001b[0m     \u001b[39m# The iterable was consumed all at once by the above for loop.\u001b[39;00m\n\u001b[0;32m   1093\u001b[0m     \u001b[39m# No need to wait for async callbacks to trigger to\u001b[39;00m\n\u001b[0;32m   1094\u001b[0m     \u001b[39m# consumption.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\soham\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\joblib\\parallel.py:901\u001b[0m, in \u001b[0;36mParallel.dispatch_one_batch\u001b[1;34m(self, iterator)\u001b[0m\n\u001b[0;32m    899\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mFalse\u001b[39;00m\n\u001b[0;32m    900\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 901\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dispatch(tasks)\n\u001b[0;32m    902\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\soham\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\joblib\\parallel.py:819\u001b[0m, in \u001b[0;36mParallel._dispatch\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    817\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n\u001b[0;32m    818\u001b[0m     job_idx \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jobs)\n\u001b[1;32m--> 819\u001b[0m     job \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_backend\u001b[39m.\u001b[39;49mapply_async(batch, callback\u001b[39m=\u001b[39;49mcb)\n\u001b[0;32m    820\u001b[0m     \u001b[39m# A job can complete so quickly than its callback is\u001b[39;00m\n\u001b[0;32m    821\u001b[0m     \u001b[39m# called before we get here, causing self._jobs to\u001b[39;00m\n\u001b[0;32m    822\u001b[0m     \u001b[39m# grow. To ensure correct results ordering, .insert is\u001b[39;00m\n\u001b[0;32m    823\u001b[0m     \u001b[39m# used (rather than .append) in the following line\u001b[39;00m\n\u001b[0;32m    824\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jobs\u001b[39m.\u001b[39minsert(job_idx, job)\n",
      "File \u001b[1;32mc:\\Users\\soham\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\joblib\\_parallel_backends.py:208\u001b[0m, in \u001b[0;36mSequentialBackend.apply_async\u001b[1;34m(self, func, callback)\u001b[0m\n\u001b[0;32m    206\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mapply_async\u001b[39m(\u001b[39mself\u001b[39m, func, callback\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m    207\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Schedule a func to be run\"\"\"\u001b[39;00m\n\u001b[1;32m--> 208\u001b[0m     result \u001b[39m=\u001b[39m ImmediateResult(func)\n\u001b[0;32m    209\u001b[0m     \u001b[39mif\u001b[39;00m callback:\n\u001b[0;32m    210\u001b[0m         callback(result)\n",
      "File \u001b[1;32mc:\\Users\\soham\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\joblib\\_parallel_backends.py:597\u001b[0m, in \u001b[0;36mImmediateResult.__init__\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    594\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, batch):\n\u001b[0;32m    595\u001b[0m     \u001b[39m# Don't delay the application, to avoid keeping the input\u001b[39;00m\n\u001b[0;32m    596\u001b[0m     \u001b[39m# arguments in memory\u001b[39;00m\n\u001b[1;32m--> 597\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mresults \u001b[39m=\u001b[39m batch()\n",
      "File \u001b[1;32mc:\\Users\\soham\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\joblib\\parallel.py:288\u001b[0m, in \u001b[0;36mBatchedCalls.__call__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    284\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    285\u001b[0m     \u001b[39m# Set the default nested backend to self._backend but do not set the\u001b[39;00m\n\u001b[0;32m    286\u001b[0m     \u001b[39m# change the default number of processes to -1\u001b[39;00m\n\u001b[0;32m    287\u001b[0m     \u001b[39mwith\u001b[39;00m parallel_backend(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backend, n_jobs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_n_jobs):\n\u001b[1;32m--> 288\u001b[0m         \u001b[39mreturn\u001b[39;00m [func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    289\u001b[0m                 \u001b[39mfor\u001b[39;00m func, args, kwargs \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mitems]\n",
      "File \u001b[1;32mc:\\Users\\soham\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\joblib\\parallel.py:288\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    284\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    285\u001b[0m     \u001b[39m# Set the default nested backend to self._backend but do not set the\u001b[39;00m\n\u001b[0;32m    286\u001b[0m     \u001b[39m# change the default number of processes to -1\u001b[39;00m\n\u001b[0;32m    287\u001b[0m     \u001b[39mwith\u001b[39;00m parallel_backend(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backend, n_jobs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_n_jobs):\n\u001b[1;32m--> 288\u001b[0m         \u001b[39mreturn\u001b[39;00m [func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    289\u001b[0m                 \u001b[39mfor\u001b[39;00m func, args, kwargs \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mitems]\n",
      "File \u001b[1;32mc:\\Users\\soham\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\parallel.py:127\u001b[0m, in \u001b[0;36m_FuncWrapper.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    125\u001b[0m     config \u001b[39m=\u001b[39m {}\n\u001b[0;32m    126\u001b[0m \u001b[39mwith\u001b[39;00m config_context(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mconfig):\n\u001b[1;32m--> 127\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfunction(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\soham\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:188\u001b[0m, in \u001b[0;36m_parallel_build_trees\u001b[1;34m(tree, bootstrap, X, y, sample_weight, tree_idx, n_trees, verbose, class_weight, n_samples_bootstrap)\u001b[0m\n\u001b[0;32m    185\u001b[0m     \u001b[39melif\u001b[39;00m class_weight \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mbalanced_subsample\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m    186\u001b[0m         curr_sample_weight \u001b[39m*\u001b[39m\u001b[39m=\u001b[39m compute_sample_weight(\u001b[39m\"\u001b[39m\u001b[39mbalanced\u001b[39m\u001b[39m\"\u001b[39m, y, indices\u001b[39m=\u001b[39mindices)\n\u001b[1;32m--> 188\u001b[0m     tree\u001b[39m.\u001b[39;49mfit(X, y, sample_weight\u001b[39m=\u001b[39;49mcurr_sample_weight, check_input\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n\u001b[0;32m    189\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    190\u001b[0m     tree\u001b[39m.\u001b[39mfit(X, y, sample_weight\u001b[39m=\u001b[39msample_weight, check_input\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\soham\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\base.py:1151\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1144\u001b[0m     estimator\u001b[39m.\u001b[39m_validate_params()\n\u001b[0;32m   1146\u001b[0m \u001b[39mwith\u001b[39;00m config_context(\n\u001b[0;32m   1147\u001b[0m     skip_parameter_validation\u001b[39m=\u001b[39m(\n\u001b[0;32m   1148\u001b[0m         prefer_skip_nested_validation \u001b[39mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1149\u001b[0m     )\n\u001b[0;32m   1150\u001b[0m ):\n\u001b[1;32m-> 1151\u001b[0m     \u001b[39mreturn\u001b[39;00m fit_method(estimator, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\soham\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\tree\\_classes.py:959\u001b[0m, in \u001b[0;36mDecisionTreeClassifier.fit\u001b[1;34m(self, X, y, sample_weight, check_input)\u001b[0m\n\u001b[0;32m    928\u001b[0m \u001b[39m@_fit_context\u001b[39m(prefer_skip_nested_validation\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m    929\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfit\u001b[39m(\u001b[39mself\u001b[39m, X, y, sample_weight\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, check_input\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m):\n\u001b[0;32m    930\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Build a decision tree classifier from the training set (X, y).\u001b[39;00m\n\u001b[0;32m    931\u001b[0m \n\u001b[0;32m    932\u001b[0m \u001b[39m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    956\u001b[0m \u001b[39m        Fitted estimator.\u001b[39;00m\n\u001b[0;32m    957\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 959\u001b[0m     \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m_fit(\n\u001b[0;32m    960\u001b[0m         X,\n\u001b[0;32m    961\u001b[0m         y,\n\u001b[0;32m    962\u001b[0m         sample_weight\u001b[39m=\u001b[39;49msample_weight,\n\u001b[0;32m    963\u001b[0m         check_input\u001b[39m=\u001b[39;49mcheck_input,\n\u001b[0;32m    964\u001b[0m     )\n\u001b[0;32m    965\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\soham\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\tree\\_classes.py:443\u001b[0m, in \u001b[0;36mBaseDecisionTree._fit\u001b[1;34m(self, X, y, sample_weight, check_input, missing_values_in_feature_mask)\u001b[0m\n\u001b[0;32m    432\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    433\u001b[0m     builder \u001b[39m=\u001b[39m BestFirstTreeBuilder(\n\u001b[0;32m    434\u001b[0m         splitter,\n\u001b[0;32m    435\u001b[0m         min_samples_split,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    440\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmin_impurity_decrease,\n\u001b[0;32m    441\u001b[0m     )\n\u001b[1;32m--> 443\u001b[0m builder\u001b[39m.\u001b[39;49mbuild(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtree_, X, y, sample_weight, missing_values_in_feature_mask)\n\u001b[0;32m    445\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_outputs_ \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m \u001b[39mand\u001b[39;00m is_classifier(\u001b[39mself\u001b[39m):\n\u001b[0;32m    446\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_classes_ \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_classes_[\u001b[39m0\u001b[39m]\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "ensemble = []\n",
    "ensemble.append(model)\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "for i in range(2):\n",
    "    rf_model = RandomForestClassifier(n_estimators=90)  \n",
    "    rf_model.fit(x_train, y_train)\n",
    "    ensemble.append(rf_model)\n",
    "    print(i,\"done\")\n",
    "    print(rf_model.score(x_test,y_test))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.evaluate(x_test,y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "875/875 [==============================] - 1s 2ms/step\n"
     ]
    }
   ],
   "source": [
    "predictions = []\n",
    "for model_pred in ensemble:\n",
    "    y_pred = model_pred.predict(x_test)\n",
    "    predictions.append(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score: 0.11235714285714286\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "#use ensemble voting to predict\n",
    "y_pred = np.zeros((len(x_test),))\n",
    "for i in range(len(x_test)):\n",
    "    counts = np.bincount([np.argmax(np.squeeze(pred[i])) for pred in predictions])\n",
    "    y_pred[i] = np.argmax(counts)\n",
    "\n",
    "# Convert y_test to multiclass format\n",
    "y_test_mc = np.argmax(y_test, axis=1)\n",
    "\n",
    "# Calculate accuracy score\n",
    "accuracy = accuracy_score(y_test_mc, y_pred)\n",
    "\n",
    "# Print accuracy score\n",
    "print(\"Accuracy score:\", accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "path=\"test.csv\"\n",
    "#do all the preprocessing done to train.csv\n",
    "df2=pd.read_csv(path)\n",
    "if 'UID' in df2.columns:\n",
    "    df2 = df2.set_index('UID')\n",
    "df2=df2.drop(columns=['ph_no','credit_card_number','email','name','url','cvv','country','job','emoji'])#does not affect the output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = r'(-?\\d+\\.\\d+)'\n",
    "\n",
    "# Use str.extract to apply the pattern column-wise\n",
    "df2 = df2.apply(lambda x: x.str.extract(pattern, expand=False))\n",
    "\n",
    "# Convert the extracted values to numeric\n",
    "df2 = df2.apply(pd.to_numeric, errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>K3Ll9</th>\n",
       "      <th>19rjS</th>\n",
       "      <th>yeIIP</th>\n",
       "      <th>Bw1V5</th>\n",
       "      <th>5k16L</th>\n",
       "      <th>e2l5S</th>\n",
       "      <th>cg31y</th>\n",
       "      <th>8SVMv</th>\n",
       "      <th>Xsi3p</th>\n",
       "      <th>l8Y6n</th>\n",
       "      <th>...</th>\n",
       "      <th>OaMqz</th>\n",
       "      <th>qhUzJ</th>\n",
       "      <th>FpCOT</th>\n",
       "      <th>zEnW3</th>\n",
       "      <th>ASDn5</th>\n",
       "      <th>vF2is</th>\n",
       "      <th>pZijn</th>\n",
       "      <th>WUc3c</th>\n",
       "      <th>sCIyG</th>\n",
       "      <th>qaERi</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>UID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>675919160</th>\n",
       "      <td>2.887040</td>\n",
       "      <td>-7.309923</td>\n",
       "      <td>19.071947</td>\n",
       "      <td>-20.583563</td>\n",
       "      <td>-3.338531</td>\n",
       "      <td>35.718716</td>\n",
       "      <td>-3.314161</td>\n",
       "      <td>12.582761</td>\n",
       "      <td>0.968303</td>\n",
       "      <td>5.687697</td>\n",
       "      <td>...</td>\n",
       "      <td>-21.248937</td>\n",
       "      <td>-5.726684</td>\n",
       "      <td>1.577796</td>\n",
       "      <td>-8.911004</td>\n",
       "      <td>5.981008</td>\n",
       "      <td>-1.297894</td>\n",
       "      <td>-1.673945</td>\n",
       "      <td>1.446459</td>\n",
       "      <td>-29.421645</td>\n",
       "      <td>1.795004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V09461652</th>\n",
       "      <td>5.654553</td>\n",
       "      <td>6.050330</td>\n",
       "      <td>2.923975</td>\n",
       "      <td>-15.514720</td>\n",
       "      <td>-0.061644</td>\n",
       "      <td>-1.254137</td>\n",
       "      <td>6.067391</td>\n",
       "      <td>3.048152</td>\n",
       "      <td>-1.617459</td>\n",
       "      <td>7.014764</td>\n",
       "      <td>...</td>\n",
       "      <td>-33.934399</td>\n",
       "      <td>-19.149704</td>\n",
       "      <td>-2.868718</td>\n",
       "      <td>15.453724</td>\n",
       "      <td>-6.211322</td>\n",
       "      <td>-0.933979</td>\n",
       "      <td>0.001376</td>\n",
       "      <td>-4.159291</td>\n",
       "      <td>-1.540406</td>\n",
       "      <td>1.296943</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>S75396644</th>\n",
       "      <td>3.053784</td>\n",
       "      <td>25.244685</td>\n",
       "      <td>6.269036</td>\n",
       "      <td>0.076592</td>\n",
       "      <td>-1.813872</td>\n",
       "      <td>43.845319</td>\n",
       "      <td>-0.819727</td>\n",
       "      <td>11.620422</td>\n",
       "      <td>-2.116804</td>\n",
       "      <td>9.157089</td>\n",
       "      <td>...</td>\n",
       "      <td>12.391218</td>\n",
       "      <td>-14.048287</td>\n",
       "      <td>-5.027959</td>\n",
       "      <td>6.754527</td>\n",
       "      <td>7.670390</td>\n",
       "      <td>0.347081</td>\n",
       "      <td>1.407364</td>\n",
       "      <td>6.091721</td>\n",
       "      <td>-22.070410</td>\n",
       "      <td>-3.748772</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>598599835</th>\n",
       "      <td>1.488450</td>\n",
       "      <td>0.283862</td>\n",
       "      <td>27.788565</td>\n",
       "      <td>28.980005</td>\n",
       "      <td>-1.681160</td>\n",
       "      <td>-1.215674</td>\n",
       "      <td>-0.598728</td>\n",
       "      <td>21.140887</td>\n",
       "      <td>3.825426</td>\n",
       "      <td>14.212199</td>\n",
       "      <td>...</td>\n",
       "      <td>11.456072</td>\n",
       "      <td>-12.459837</td>\n",
       "      <td>-3.448771</td>\n",
       "      <td>20.407682</td>\n",
       "      <td>-1.601812</td>\n",
       "      <td>1.355095</td>\n",
       "      <td>0.979308</td>\n",
       "      <td>-1.409996</td>\n",
       "      <td>2.120580</td>\n",
       "      <td>-3.924106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>W60397022</th>\n",
       "      <td>10.885250</td>\n",
       "      <td>-22.040204</td>\n",
       "      <td>-7.699833</td>\n",
       "      <td>22.169769</td>\n",
       "      <td>6.763151</td>\n",
       "      <td>14.833072</td>\n",
       "      <td>-6.158491</td>\n",
       "      <td>20.194413</td>\n",
       "      <td>0.961350</td>\n",
       "      <td>0.608986</td>\n",
       "      <td>...</td>\n",
       "      <td>19.865807</td>\n",
       "      <td>-24.537809</td>\n",
       "      <td>1.106189</td>\n",
       "      <td>-14.192347</td>\n",
       "      <td>3.930237</td>\n",
       "      <td>9.832580</td>\n",
       "      <td>-0.672717</td>\n",
       "      <td>-7.779142</td>\n",
       "      <td>15.756719</td>\n",
       "      <td>-7.693867</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>G18996472</th>\n",
       "      <td>-3.449114</td>\n",
       "      <td>-7.364562</td>\n",
       "      <td>-12.543183</td>\n",
       "      <td>-33.475045</td>\n",
       "      <td>1.433123</td>\n",
       "      <td>7.984281</td>\n",
       "      <td>-3.486359</td>\n",
       "      <td>26.646217</td>\n",
       "      <td>-1.562326</td>\n",
       "      <td>1.489204</td>\n",
       "      <td>...</td>\n",
       "      <td>19.649591</td>\n",
       "      <td>-8.807412</td>\n",
       "      <td>8.620351</td>\n",
       "      <td>-1.111601</td>\n",
       "      <td>2.367149</td>\n",
       "      <td>-1.469333</td>\n",
       "      <td>1.480501</td>\n",
       "      <td>2.712732</td>\n",
       "      <td>-8.616272</td>\n",
       "      <td>-7.006872</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119545284</th>\n",
       "      <td>-2.510711</td>\n",
       "      <td>-13.983728</td>\n",
       "      <td>33.684391</td>\n",
       "      <td>-1.341565</td>\n",
       "      <td>2.141081</td>\n",
       "      <td>30.300519</td>\n",
       "      <td>-7.994206</td>\n",
       "      <td>10.898682</td>\n",
       "      <td>-4.059479</td>\n",
       "      <td>12.568448</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-39.585549</td>\n",
       "      <td>2.507032</td>\n",
       "      <td>1.835891</td>\n",
       "      <td>-0.362596</td>\n",
       "      <td>3.059471</td>\n",
       "      <td>3.154485</td>\n",
       "      <td>1.802282</td>\n",
       "      <td>4.723408</td>\n",
       "      <td>-4.572501</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Q57447995</th>\n",
       "      <td>4.856940</td>\n",
       "      <td>-14.733054</td>\n",
       "      <td>-1.945138</td>\n",
       "      <td>13.639729</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-27.256234</td>\n",
       "      <td>-5.715546</td>\n",
       "      <td>3.610817</td>\n",
       "      <td>6.290513</td>\n",
       "      <td>14.360807</td>\n",
       "      <td>...</td>\n",
       "      <td>32.368422</td>\n",
       "      <td>-19.829218</td>\n",
       "      <td>5.767188</td>\n",
       "      <td>-18.795186</td>\n",
       "      <td>-9.603047</td>\n",
       "      <td>-3.901352</td>\n",
       "      <td>3.412305</td>\n",
       "      <td>4.303302</td>\n",
       "      <td>-23.636226</td>\n",
       "      <td>0.553597</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>611053577</th>\n",
       "      <td>-6.331716</td>\n",
       "      <td>-23.445389</td>\n",
       "      <td>-6.378595</td>\n",
       "      <td>14.767969</td>\n",
       "      <td>0.121648</td>\n",
       "      <td>7.185791</td>\n",
       "      <td>-12.596212</td>\n",
       "      <td>28.909433</td>\n",
       "      <td>5.870233</td>\n",
       "      <td>-17.737648</td>\n",
       "      <td>...</td>\n",
       "      <td>-17.347608</td>\n",
       "      <td>14.993694</td>\n",
       "      <td>2.471468</td>\n",
       "      <td>-23.408356</td>\n",
       "      <td>-3.488413</td>\n",
       "      <td>-5.711458</td>\n",
       "      <td>2.541118</td>\n",
       "      <td>-3.005590</td>\n",
       "      <td>4.464814</td>\n",
       "      <td>-2.205268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>C13097269</th>\n",
       "      <td>-2.777361</td>\n",
       "      <td>-3.632092</td>\n",
       "      <td>-39.794629</td>\n",
       "      <td>9.588133</td>\n",
       "      <td>1.099502</td>\n",
       "      <td>4.903521</td>\n",
       "      <td>-1.584658</td>\n",
       "      <td>31.912007</td>\n",
       "      <td>-4.414914</td>\n",
       "      <td>-32.612675</td>\n",
       "      <td>...</td>\n",
       "      <td>9.144159</td>\n",
       "      <td>-15.927747</td>\n",
       "      <td>1.628786</td>\n",
       "      <td>-16.504059</td>\n",
       "      <td>2.623832</td>\n",
       "      <td>-5.564953</td>\n",
       "      <td>-2.776443</td>\n",
       "      <td>8.154846</td>\n",
       "      <td>-41.071138</td>\n",
       "      <td>-0.330314</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>60000 rows Ã— 100 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               K3Ll9      19rjS      yeIIP      Bw1V5     5k16L      e2l5S  \\\n",
       "UID                                                                          \n",
       "675919160   2.887040  -7.309923  19.071947 -20.583563 -3.338531  35.718716   \n",
       "V09461652   5.654553   6.050330   2.923975 -15.514720 -0.061644  -1.254137   \n",
       "S75396644   3.053784  25.244685   6.269036   0.076592 -1.813872  43.845319   \n",
       "598599835   1.488450   0.283862  27.788565  28.980005 -1.681160  -1.215674   \n",
       "W60397022  10.885250 -22.040204  -7.699833  22.169769  6.763151  14.833072   \n",
       "...              ...        ...        ...        ...       ...        ...   \n",
       "G18996472  -3.449114  -7.364562 -12.543183 -33.475045  1.433123   7.984281   \n",
       "119545284  -2.510711 -13.983728  33.684391  -1.341565  2.141081  30.300519   \n",
       "Q57447995   4.856940 -14.733054  -1.945138  13.639729       NaN -27.256234   \n",
       "611053577  -6.331716 -23.445389  -6.378595  14.767969  0.121648   7.185791   \n",
       "C13097269  -2.777361  -3.632092 -39.794629   9.588133  1.099502   4.903521   \n",
       "\n",
       "               cg31y      8SVMv     Xsi3p      l8Y6n  ...      OaMqz  \\\n",
       "UID                                                   ...              \n",
       "675919160  -3.314161  12.582761  0.968303   5.687697  ... -21.248937   \n",
       "V09461652   6.067391   3.048152 -1.617459   7.014764  ... -33.934399   \n",
       "S75396644  -0.819727  11.620422 -2.116804   9.157089  ...  12.391218   \n",
       "598599835  -0.598728  21.140887  3.825426  14.212199  ...  11.456072   \n",
       "W60397022  -6.158491  20.194413  0.961350   0.608986  ...  19.865807   \n",
       "...              ...        ...       ...        ...  ...        ...   \n",
       "G18996472  -3.486359  26.646217 -1.562326   1.489204  ...  19.649591   \n",
       "119545284  -7.994206  10.898682 -4.059479  12.568448  ...        NaN   \n",
       "Q57447995  -5.715546   3.610817  6.290513  14.360807  ...  32.368422   \n",
       "611053577 -12.596212  28.909433  5.870233 -17.737648  ... -17.347608   \n",
       "C13097269  -1.584658  31.912007 -4.414914 -32.612675  ...   9.144159   \n",
       "\n",
       "               qhUzJ     FpCOT      zEnW3     ASDn5     vF2is     pZijn  \\\n",
       "UID                                                                       \n",
       "675919160  -5.726684  1.577796  -8.911004  5.981008 -1.297894 -1.673945   \n",
       "V09461652 -19.149704 -2.868718  15.453724 -6.211322 -0.933979  0.001376   \n",
       "S75396644 -14.048287 -5.027959   6.754527  7.670390  0.347081  1.407364   \n",
       "598599835 -12.459837 -3.448771  20.407682 -1.601812  1.355095  0.979308   \n",
       "W60397022 -24.537809  1.106189 -14.192347  3.930237  9.832580 -0.672717   \n",
       "...              ...       ...        ...       ...       ...       ...   \n",
       "G18996472  -8.807412  8.620351  -1.111601  2.367149 -1.469333  1.480501   \n",
       "119545284 -39.585549  2.507032   1.835891 -0.362596  3.059471  3.154485   \n",
       "Q57447995 -19.829218  5.767188 -18.795186 -9.603047 -3.901352  3.412305   \n",
       "611053577  14.993694  2.471468 -23.408356 -3.488413 -5.711458  2.541118   \n",
       "C13097269 -15.927747  1.628786 -16.504059  2.623832 -5.564953 -2.776443   \n",
       "\n",
       "              WUc3c      sCIyG     qaERi  \n",
       "UID                                       \n",
       "675919160  1.446459 -29.421645  1.795004  \n",
       "V09461652 -4.159291  -1.540406  1.296943  \n",
       "S75396644  6.091721 -22.070410 -3.748772  \n",
       "598599835 -1.409996   2.120580 -3.924106  \n",
       "W60397022 -7.779142  15.756719 -7.693867  \n",
       "...             ...        ...       ...  \n",
       "G18996472  2.712732  -8.616272 -7.006872  \n",
       "119545284  1.802282   4.723408 -4.572501  \n",
       "Q57447995  4.303302 -23.636226  0.553597  \n",
       "611053577 -3.005590   4.464814 -2.205268  \n",
       "C13097269  8.154846 -41.071138 -0.330314  \n",
       "\n",
       "[60000 rows x 100 columns]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df2_imputed = df2.fillna(df.median())\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "imputer = SimpleImputer(strategy='median')\n",
    "imputed_data_2= imputer.fit_transform(df2)\n",
    "\n",
    "from sklearn.preprocessing import QuantileTransformer\n",
    "\n",
    "\n",
    "transformer = QuantileTransformer(n_quantiles=1000, output_distribution='uniform')\n",
    "\n",
    "# Fit and transform the entire dataset\n",
    "transformed_data_2 = transformer.fit_transform(imputed_data_2)\n",
    "\n",
    "# Convert the transformed data back to a DataFrame\n",
    "df2_imputed= pd.DataFrame(transformed_data_2, columns=df2.columns)\n",
    "#set id as uid\n",
    "df2_imputed['UID']=df2.index\n",
    "df2_imputed=df2_imputed.set_index('UID')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>K3Ll9</th>\n",
       "      <th>19rjS</th>\n",
       "      <th>yeIIP</th>\n",
       "      <th>Bw1V5</th>\n",
       "      <th>5k16L</th>\n",
       "      <th>e2l5S</th>\n",
       "      <th>cg31y</th>\n",
       "      <th>8SVMv</th>\n",
       "      <th>Xsi3p</th>\n",
       "      <th>l8Y6n</th>\n",
       "      <th>...</th>\n",
       "      <th>OaMqz</th>\n",
       "      <th>qhUzJ</th>\n",
       "      <th>FpCOT</th>\n",
       "      <th>zEnW3</th>\n",
       "      <th>ASDn5</th>\n",
       "      <th>vF2is</th>\n",
       "      <th>pZijn</th>\n",
       "      <th>WUc3c</th>\n",
       "      <th>sCIyG</th>\n",
       "      <th>qaERi</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>UID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>675919160</th>\n",
       "      <td>0.714512</td>\n",
       "      <td>0.316307</td>\n",
       "      <td>0.849243</td>\n",
       "      <td>0.113108</td>\n",
       "      <td>0.210281</td>\n",
       "      <td>0.963563</td>\n",
       "      <td>0.214255</td>\n",
       "      <td>0.771249</td>\n",
       "      <td>0.597734</td>\n",
       "      <td>0.642110</td>\n",
       "      <td>...</td>\n",
       "      <td>0.124366</td>\n",
       "      <td>0.388033</td>\n",
       "      <td>0.662555</td>\n",
       "      <td>0.292214</td>\n",
       "      <td>0.929992</td>\n",
       "      <td>0.390003</td>\n",
       "      <td>0.307467</td>\n",
       "      <td>0.635848</td>\n",
       "      <td>0.059355</td>\n",
       "      <td>0.679244</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V09461652</th>\n",
       "      <td>0.891160</td>\n",
       "      <td>0.629012</td>\n",
       "      <td>0.570808</td>\n",
       "      <td>0.182308</td>\n",
       "      <td>0.488176</td>\n",
       "      <td>0.450291</td>\n",
       "      <td>0.932969</td>\n",
       "      <td>0.585599</td>\n",
       "      <td>0.359676</td>\n",
       "      <td>0.673772</td>\n",
       "      <td>...</td>\n",
       "      <td>0.034788</td>\n",
       "      <td>0.164027</td>\n",
       "      <td>0.253903</td>\n",
       "      <td>0.861488</td>\n",
       "      <td>0.080164</td>\n",
       "      <td>0.428924</td>\n",
       "      <td>0.460434</td>\n",
       "      <td>0.171017</td>\n",
       "      <td>0.452404</td>\n",
       "      <td>0.634508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>S75396644</th>\n",
       "      <td>0.727341</td>\n",
       "      <td>0.927531</td>\n",
       "      <td>0.638334</td>\n",
       "      <td>0.511807</td>\n",
       "      <td>0.325005</td>\n",
       "      <td>0.987632</td>\n",
       "      <td>0.425000</td>\n",
       "      <td>0.754368</td>\n",
       "      <td>0.317235</td>\n",
       "      <td>0.716563</td>\n",
       "      <td>...</td>\n",
       "      <td>0.701889</td>\n",
       "      <td>0.235279</td>\n",
       "      <td>0.119855</td>\n",
       "      <td>0.698082</td>\n",
       "      <td>0.970180</td>\n",
       "      <td>0.560332</td>\n",
       "      <td>0.605711</td>\n",
       "      <td>0.926858</td>\n",
       "      <td>0.119394</td>\n",
       "      <td>0.178447</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>598599835</th>\n",
       "      <td>0.597541</td>\n",
       "      <td>0.489017</td>\n",
       "      <td>0.932068</td>\n",
       "      <td>0.955257</td>\n",
       "      <td>0.337333</td>\n",
       "      <td>0.451146</td>\n",
       "      <td>0.444093</td>\n",
       "      <td>0.885519</td>\n",
       "      <td>0.825569</td>\n",
       "      <td>0.805075</td>\n",
       "      <td>...</td>\n",
       "      <td>0.683355</td>\n",
       "      <td>0.262092</td>\n",
       "      <td>0.210795</td>\n",
       "      <td>0.922379</td>\n",
       "      <td>0.370258</td>\n",
       "      <td>0.654968</td>\n",
       "      <td>0.568740</td>\n",
       "      <td>0.376885</td>\n",
       "      <td>0.538039</td>\n",
       "      <td>0.164508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>W60397022</th>\n",
       "      <td>0.992638</td>\n",
       "      <td>0.095270</td>\n",
       "      <td>0.335081</td>\n",
       "      <td>0.906657</td>\n",
       "      <td>0.941041</td>\n",
       "      <td>0.766449</td>\n",
       "      <td>0.068666</td>\n",
       "      <td>0.876243</td>\n",
       "      <td>0.597069</td>\n",
       "      <td>0.527452</td>\n",
       "      <td>...</td>\n",
       "      <td>0.820018</td>\n",
       "      <td>0.105916</td>\n",
       "      <td>0.621004</td>\n",
       "      <td>0.180551</td>\n",
       "      <td>0.835095</td>\n",
       "      <td>0.992785</td>\n",
       "      <td>0.396695</td>\n",
       "      <td>0.035373</td>\n",
       "      <td>0.787513</td>\n",
       "      <td>0.029676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>G18996472</th>\n",
       "      <td>0.183167</td>\n",
       "      <td>0.315152</td>\n",
       "      <td>0.243894</td>\n",
       "      <td>0.023551</td>\n",
       "      <td>0.633553</td>\n",
       "      <td>0.643019</td>\n",
       "      <td>0.203701</td>\n",
       "      <td>0.936778</td>\n",
       "      <td>0.364859</td>\n",
       "      <td>0.548150</td>\n",
       "      <td>...</td>\n",
       "      <td>0.816817</td>\n",
       "      <td>0.331017</td>\n",
       "      <td>0.983051</td>\n",
       "      <td>0.483149</td>\n",
       "      <td>0.730151</td>\n",
       "      <td>0.374854</td>\n",
       "      <td>0.612365</td>\n",
       "      <td>0.740714</td>\n",
       "      <td>0.311810</td>\n",
       "      <td>0.042603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119545284</th>\n",
       "      <td>0.251341</td>\n",
       "      <td>0.192358</td>\n",
       "      <td>0.965875</td>\n",
       "      <td>0.470700</td>\n",
       "      <td>0.690746</td>\n",
       "      <td>0.937005</td>\n",
       "      <td>0.027848</td>\n",
       "      <td>0.741450</td>\n",
       "      <td>0.176143</td>\n",
       "      <td>0.778384</td>\n",
       "      <td>...</td>\n",
       "      <td>0.492993</td>\n",
       "      <td>0.020586</td>\n",
       "      <td>0.734269</td>\n",
       "      <td>0.574344</td>\n",
       "      <td>0.484472</td>\n",
       "      <td>0.790303</td>\n",
       "      <td>0.757595</td>\n",
       "      <td>0.666004</td>\n",
       "      <td>0.590182</td>\n",
       "      <td>0.129848</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Q57447995</th>\n",
       "      <td>0.849089</td>\n",
       "      <td>0.183092</td>\n",
       "      <td>0.458299</td>\n",
       "      <td>0.798608</td>\n",
       "      <td>0.506006</td>\n",
       "      <td>0.069732</td>\n",
       "      <td>0.082473</td>\n",
       "      <td>0.597583</td>\n",
       "      <td>0.932051</td>\n",
       "      <td>0.806826</td>\n",
       "      <td>...</td>\n",
       "      <td>0.939278</td>\n",
       "      <td>0.155785</td>\n",
       "      <td>0.917640</td>\n",
       "      <td>0.112279</td>\n",
       "      <td>0.012447</td>\n",
       "      <td>0.181358</td>\n",
       "      <td>0.777125</td>\n",
       "      <td>0.845064</td>\n",
       "      <td>0.104555</td>\n",
       "      <td>0.569813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>611053577</th>\n",
       "      <td>0.055636</td>\n",
       "      <td>0.085746</td>\n",
       "      <td>0.362893</td>\n",
       "      <td>0.814742</td>\n",
       "      <td>0.513670</td>\n",
       "      <td>0.627730</td>\n",
       "      <td>0.001399</td>\n",
       "      <td>0.950650</td>\n",
       "      <td>0.920209</td>\n",
       "      <td>0.149938</td>\n",
       "      <td>...</td>\n",
       "      <td>0.163714</td>\n",
       "      <td>0.793421</td>\n",
       "      <td>0.731699</td>\n",
       "      <td>0.069229</td>\n",
       "      <td>0.217535</td>\n",
       "      <td>0.090502</td>\n",
       "      <td>0.708944</td>\n",
       "      <td>0.246887</td>\n",
       "      <td>0.585255</td>\n",
       "      <td>0.301478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>C13097269</th>\n",
       "      <td>0.229576</td>\n",
       "      <td>0.397542</td>\n",
       "      <td>0.017238</td>\n",
       "      <td>0.721747</td>\n",
       "      <td>0.602984</td>\n",
       "      <td>0.586547</td>\n",
       "      <td>0.356510</td>\n",
       "      <td>0.964053</td>\n",
       "      <td>0.156462</td>\n",
       "      <td>0.031808</td>\n",
       "      <td>...</td>\n",
       "      <td>0.641796</td>\n",
       "      <td>0.206206</td>\n",
       "      <td>0.666617</td>\n",
       "      <td>0.142469</td>\n",
       "      <td>0.747868</td>\n",
       "      <td>0.096059</td>\n",
       "      <td>0.221303</td>\n",
       "      <td>0.975673</td>\n",
       "      <td>0.016536</td>\n",
       "      <td>0.475805</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>60000 rows Ã— 100 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              K3Ll9     19rjS     yeIIP     Bw1V5     5k16L     e2l5S  \\\n",
       "UID                                                                     \n",
       "675919160  0.714512  0.316307  0.849243  0.113108  0.210281  0.963563   \n",
       "V09461652  0.891160  0.629012  0.570808  0.182308  0.488176  0.450291   \n",
       "S75396644  0.727341  0.927531  0.638334  0.511807  0.325005  0.987632   \n",
       "598599835  0.597541  0.489017  0.932068  0.955257  0.337333  0.451146   \n",
       "W60397022  0.992638  0.095270  0.335081  0.906657  0.941041  0.766449   \n",
       "...             ...       ...       ...       ...       ...       ...   \n",
       "G18996472  0.183167  0.315152  0.243894  0.023551  0.633553  0.643019   \n",
       "119545284  0.251341  0.192358  0.965875  0.470700  0.690746  0.937005   \n",
       "Q57447995  0.849089  0.183092  0.458299  0.798608  0.506006  0.069732   \n",
       "611053577  0.055636  0.085746  0.362893  0.814742  0.513670  0.627730   \n",
       "C13097269  0.229576  0.397542  0.017238  0.721747  0.602984  0.586547   \n",
       "\n",
       "              cg31y     8SVMv     Xsi3p     l8Y6n  ...     OaMqz     qhUzJ  \\\n",
       "UID                                                ...                       \n",
       "675919160  0.214255  0.771249  0.597734  0.642110  ...  0.124366  0.388033   \n",
       "V09461652  0.932969  0.585599  0.359676  0.673772  ...  0.034788  0.164027   \n",
       "S75396644  0.425000  0.754368  0.317235  0.716563  ...  0.701889  0.235279   \n",
       "598599835  0.444093  0.885519  0.825569  0.805075  ...  0.683355  0.262092   \n",
       "W60397022  0.068666  0.876243  0.597069  0.527452  ...  0.820018  0.105916   \n",
       "...             ...       ...       ...       ...  ...       ...       ...   \n",
       "G18996472  0.203701  0.936778  0.364859  0.548150  ...  0.816817  0.331017   \n",
       "119545284  0.027848  0.741450  0.176143  0.778384  ...  0.492993  0.020586   \n",
       "Q57447995  0.082473  0.597583  0.932051  0.806826  ...  0.939278  0.155785   \n",
       "611053577  0.001399  0.950650  0.920209  0.149938  ...  0.163714  0.793421   \n",
       "C13097269  0.356510  0.964053  0.156462  0.031808  ...  0.641796  0.206206   \n",
       "\n",
       "              FpCOT     zEnW3     ASDn5     vF2is     pZijn     WUc3c  \\\n",
       "UID                                                                     \n",
       "675919160  0.662555  0.292214  0.929992  0.390003  0.307467  0.635848   \n",
       "V09461652  0.253903  0.861488  0.080164  0.428924  0.460434  0.171017   \n",
       "S75396644  0.119855  0.698082  0.970180  0.560332  0.605711  0.926858   \n",
       "598599835  0.210795  0.922379  0.370258  0.654968  0.568740  0.376885   \n",
       "W60397022  0.621004  0.180551  0.835095  0.992785  0.396695  0.035373   \n",
       "...             ...       ...       ...       ...       ...       ...   \n",
       "G18996472  0.983051  0.483149  0.730151  0.374854  0.612365  0.740714   \n",
       "119545284  0.734269  0.574344  0.484472  0.790303  0.757595  0.666004   \n",
       "Q57447995  0.917640  0.112279  0.012447  0.181358  0.777125  0.845064   \n",
       "611053577  0.731699  0.069229  0.217535  0.090502  0.708944  0.246887   \n",
       "C13097269  0.666617  0.142469  0.747868  0.096059  0.221303  0.975673   \n",
       "\n",
       "              sCIyG     qaERi  \n",
       "UID                            \n",
       "675919160  0.059355  0.679244  \n",
       "V09461652  0.452404  0.634508  \n",
       "S75396644  0.119394  0.178447  \n",
       "598599835  0.538039  0.164508  \n",
       "W60397022  0.787513  0.029676  \n",
       "...             ...       ...  \n",
       "G18996472  0.311810  0.042603  \n",
       "119545284  0.590182  0.129848  \n",
       "Q57447995  0.104555  0.569813  \n",
       "611053577  0.585255  0.301478  \n",
       "C13097269  0.016536  0.475805  \n",
       "\n",
       "[60000 rows x 100 columns]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2_imputed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2_imputed_dropped=df2_imputed.drop(columns=to_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>K3Ll9</th>\n",
       "      <th>19rjS</th>\n",
       "      <th>yeIIP</th>\n",
       "      <th>Bw1V5</th>\n",
       "      <th>5k16L</th>\n",
       "      <th>e2l5S</th>\n",
       "      <th>cg31y</th>\n",
       "      <th>8SVMv</th>\n",
       "      <th>Xsi3p</th>\n",
       "      <th>l8Y6n</th>\n",
       "      <th>...</th>\n",
       "      <th>Bz7Ov</th>\n",
       "      <th>uN0aA</th>\n",
       "      <th>OaMqz</th>\n",
       "      <th>qhUzJ</th>\n",
       "      <th>FpCOT</th>\n",
       "      <th>zEnW3</th>\n",
       "      <th>ASDn5</th>\n",
       "      <th>vF2is</th>\n",
       "      <th>sCIyG</th>\n",
       "      <th>qaERi</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>UID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>675919160</th>\n",
       "      <td>0.714512</td>\n",
       "      <td>0.316307</td>\n",
       "      <td>0.849243</td>\n",
       "      <td>0.113108</td>\n",
       "      <td>0.210281</td>\n",
       "      <td>0.963563</td>\n",
       "      <td>0.214255</td>\n",
       "      <td>0.771249</td>\n",
       "      <td>0.597734</td>\n",
       "      <td>0.642110</td>\n",
       "      <td>...</td>\n",
       "      <td>0.181106</td>\n",
       "      <td>0.860022</td>\n",
       "      <td>0.124366</td>\n",
       "      <td>0.388033</td>\n",
       "      <td>0.662555</td>\n",
       "      <td>0.292214</td>\n",
       "      <td>0.929992</td>\n",
       "      <td>0.390003</td>\n",
       "      <td>0.059355</td>\n",
       "      <td>0.679244</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V09461652</th>\n",
       "      <td>0.891160</td>\n",
       "      <td>0.629012</td>\n",
       "      <td>0.570808</td>\n",
       "      <td>0.182308</td>\n",
       "      <td>0.488176</td>\n",
       "      <td>0.450291</td>\n",
       "      <td>0.932969</td>\n",
       "      <td>0.585599</td>\n",
       "      <td>0.359676</td>\n",
       "      <td>0.673772</td>\n",
       "      <td>...</td>\n",
       "      <td>0.036945</td>\n",
       "      <td>0.550707</td>\n",
       "      <td>0.034788</td>\n",
       "      <td>0.164027</td>\n",
       "      <td>0.253903</td>\n",
       "      <td>0.861488</td>\n",
       "      <td>0.080164</td>\n",
       "      <td>0.428924</td>\n",
       "      <td>0.452404</td>\n",
       "      <td>0.634508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>S75396644</th>\n",
       "      <td>0.727341</td>\n",
       "      <td>0.927531</td>\n",
       "      <td>0.638334</td>\n",
       "      <td>0.511807</td>\n",
       "      <td>0.325005</td>\n",
       "      <td>0.987632</td>\n",
       "      <td>0.425000</td>\n",
       "      <td>0.754368</td>\n",
       "      <td>0.317235</td>\n",
       "      <td>0.716563</td>\n",
       "      <td>...</td>\n",
       "      <td>0.473936</td>\n",
       "      <td>0.312972</td>\n",
       "      <td>0.701889</td>\n",
       "      <td>0.235279</td>\n",
       "      <td>0.119855</td>\n",
       "      <td>0.698082</td>\n",
       "      <td>0.970180</td>\n",
       "      <td>0.560332</td>\n",
       "      <td>0.119394</td>\n",
       "      <td>0.178447</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>598599835</th>\n",
       "      <td>0.597541</td>\n",
       "      <td>0.489017</td>\n",
       "      <td>0.932068</td>\n",
       "      <td>0.955257</td>\n",
       "      <td>0.337333</td>\n",
       "      <td>0.451146</td>\n",
       "      <td>0.444093</td>\n",
       "      <td>0.885519</td>\n",
       "      <td>0.825569</td>\n",
       "      <td>0.805075</td>\n",
       "      <td>...</td>\n",
       "      <td>0.350716</td>\n",
       "      <td>0.867604</td>\n",
       "      <td>0.683355</td>\n",
       "      <td>0.262092</td>\n",
       "      <td>0.210795</td>\n",
       "      <td>0.922379</td>\n",
       "      <td>0.370258</td>\n",
       "      <td>0.654968</td>\n",
       "      <td>0.538039</td>\n",
       "      <td>0.164508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>W60397022</th>\n",
       "      <td>0.992638</td>\n",
       "      <td>0.095270</td>\n",
       "      <td>0.335081</td>\n",
       "      <td>0.906657</td>\n",
       "      <td>0.941041</td>\n",
       "      <td>0.766449</td>\n",
       "      <td>0.068666</td>\n",
       "      <td>0.876243</td>\n",
       "      <td>0.597069</td>\n",
       "      <td>0.527452</td>\n",
       "      <td>...</td>\n",
       "      <td>0.449298</td>\n",
       "      <td>0.924544</td>\n",
       "      <td>0.820018</td>\n",
       "      <td>0.105916</td>\n",
       "      <td>0.621004</td>\n",
       "      <td>0.180551</td>\n",
       "      <td>0.835095</td>\n",
       "      <td>0.992785</td>\n",
       "      <td>0.787513</td>\n",
       "      <td>0.029676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>G18996472</th>\n",
       "      <td>0.183167</td>\n",
       "      <td>0.315152</td>\n",
       "      <td>0.243894</td>\n",
       "      <td>0.023551</td>\n",
       "      <td>0.633553</td>\n",
       "      <td>0.643019</td>\n",
       "      <td>0.203701</td>\n",
       "      <td>0.936778</td>\n",
       "      <td>0.364859</td>\n",
       "      <td>0.548150</td>\n",
       "      <td>...</td>\n",
       "      <td>0.450722</td>\n",
       "      <td>0.995231</td>\n",
       "      <td>0.816817</td>\n",
       "      <td>0.331017</td>\n",
       "      <td>0.983051</td>\n",
       "      <td>0.483149</td>\n",
       "      <td>0.730151</td>\n",
       "      <td>0.374854</td>\n",
       "      <td>0.311810</td>\n",
       "      <td>0.042603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119545284</th>\n",
       "      <td>0.251341</td>\n",
       "      <td>0.192358</td>\n",
       "      <td>0.965875</td>\n",
       "      <td>0.470700</td>\n",
       "      <td>0.690746</td>\n",
       "      <td>0.937005</td>\n",
       "      <td>0.027848</td>\n",
       "      <td>0.741450</td>\n",
       "      <td>0.176143</td>\n",
       "      <td>0.778384</td>\n",
       "      <td>...</td>\n",
       "      <td>0.639774</td>\n",
       "      <td>0.872179</td>\n",
       "      <td>0.492993</td>\n",
       "      <td>0.020586</td>\n",
       "      <td>0.734269</td>\n",
       "      <td>0.574344</td>\n",
       "      <td>0.484472</td>\n",
       "      <td>0.790303</td>\n",
       "      <td>0.590182</td>\n",
       "      <td>0.129848</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Q57447995</th>\n",
       "      <td>0.849089</td>\n",
       "      <td>0.183092</td>\n",
       "      <td>0.458299</td>\n",
       "      <td>0.798608</td>\n",
       "      <td>0.506006</td>\n",
       "      <td>0.069732</td>\n",
       "      <td>0.082473</td>\n",
       "      <td>0.597583</td>\n",
       "      <td>0.932051</td>\n",
       "      <td>0.806826</td>\n",
       "      <td>...</td>\n",
       "      <td>0.076263</td>\n",
       "      <td>0.299759</td>\n",
       "      <td>0.939278</td>\n",
       "      <td>0.155785</td>\n",
       "      <td>0.917640</td>\n",
       "      <td>0.112279</td>\n",
       "      <td>0.012447</td>\n",
       "      <td>0.181358</td>\n",
       "      <td>0.104555</td>\n",
       "      <td>0.569813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>611053577</th>\n",
       "      <td>0.055636</td>\n",
       "      <td>0.085746</td>\n",
       "      <td>0.362893</td>\n",
       "      <td>0.814742</td>\n",
       "      <td>0.513670</td>\n",
       "      <td>0.627730</td>\n",
       "      <td>0.001399</td>\n",
       "      <td>0.950650</td>\n",
       "      <td>0.920209</td>\n",
       "      <td>0.149938</td>\n",
       "      <td>...</td>\n",
       "      <td>0.553554</td>\n",
       "      <td>0.662845</td>\n",
       "      <td>0.163714</td>\n",
       "      <td>0.793421</td>\n",
       "      <td>0.731699</td>\n",
       "      <td>0.069229</td>\n",
       "      <td>0.217535</td>\n",
       "      <td>0.090502</td>\n",
       "      <td>0.585255</td>\n",
       "      <td>0.301478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>C13097269</th>\n",
       "      <td>0.229576</td>\n",
       "      <td>0.397542</td>\n",
       "      <td>0.017238</td>\n",
       "      <td>0.721747</td>\n",
       "      <td>0.602984</td>\n",
       "      <td>0.586547</td>\n",
       "      <td>0.356510</td>\n",
       "      <td>0.964053</td>\n",
       "      <td>0.156462</td>\n",
       "      <td>0.031808</td>\n",
       "      <td>...</td>\n",
       "      <td>0.853805</td>\n",
       "      <td>0.360736</td>\n",
       "      <td>0.641796</td>\n",
       "      <td>0.206206</td>\n",
       "      <td>0.666617</td>\n",
       "      <td>0.142469</td>\n",
       "      <td>0.747868</td>\n",
       "      <td>0.096059</td>\n",
       "      <td>0.016536</td>\n",
       "      <td>0.475805</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>60000 rows Ã— 92 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              K3Ll9     19rjS     yeIIP     Bw1V5     5k16L     e2l5S  \\\n",
       "UID                                                                     \n",
       "675919160  0.714512  0.316307  0.849243  0.113108  0.210281  0.963563   \n",
       "V09461652  0.891160  0.629012  0.570808  0.182308  0.488176  0.450291   \n",
       "S75396644  0.727341  0.927531  0.638334  0.511807  0.325005  0.987632   \n",
       "598599835  0.597541  0.489017  0.932068  0.955257  0.337333  0.451146   \n",
       "W60397022  0.992638  0.095270  0.335081  0.906657  0.941041  0.766449   \n",
       "...             ...       ...       ...       ...       ...       ...   \n",
       "G18996472  0.183167  0.315152  0.243894  0.023551  0.633553  0.643019   \n",
       "119545284  0.251341  0.192358  0.965875  0.470700  0.690746  0.937005   \n",
       "Q57447995  0.849089  0.183092  0.458299  0.798608  0.506006  0.069732   \n",
       "611053577  0.055636  0.085746  0.362893  0.814742  0.513670  0.627730   \n",
       "C13097269  0.229576  0.397542  0.017238  0.721747  0.602984  0.586547   \n",
       "\n",
       "              cg31y     8SVMv     Xsi3p     l8Y6n  ...     Bz7Ov     uN0aA  \\\n",
       "UID                                                ...                       \n",
       "675919160  0.214255  0.771249  0.597734  0.642110  ...  0.181106  0.860022   \n",
       "V09461652  0.932969  0.585599  0.359676  0.673772  ...  0.036945  0.550707   \n",
       "S75396644  0.425000  0.754368  0.317235  0.716563  ...  0.473936  0.312972   \n",
       "598599835  0.444093  0.885519  0.825569  0.805075  ...  0.350716  0.867604   \n",
       "W60397022  0.068666  0.876243  0.597069  0.527452  ...  0.449298  0.924544   \n",
       "...             ...       ...       ...       ...  ...       ...       ...   \n",
       "G18996472  0.203701  0.936778  0.364859  0.548150  ...  0.450722  0.995231   \n",
       "119545284  0.027848  0.741450  0.176143  0.778384  ...  0.639774  0.872179   \n",
       "Q57447995  0.082473  0.597583  0.932051  0.806826  ...  0.076263  0.299759   \n",
       "611053577  0.001399  0.950650  0.920209  0.149938  ...  0.553554  0.662845   \n",
       "C13097269  0.356510  0.964053  0.156462  0.031808  ...  0.853805  0.360736   \n",
       "\n",
       "              OaMqz     qhUzJ     FpCOT     zEnW3     ASDn5     vF2is  \\\n",
       "UID                                                                     \n",
       "675919160  0.124366  0.388033  0.662555  0.292214  0.929992  0.390003   \n",
       "V09461652  0.034788  0.164027  0.253903  0.861488  0.080164  0.428924   \n",
       "S75396644  0.701889  0.235279  0.119855  0.698082  0.970180  0.560332   \n",
       "598599835  0.683355  0.262092  0.210795  0.922379  0.370258  0.654968   \n",
       "W60397022  0.820018  0.105916  0.621004  0.180551  0.835095  0.992785   \n",
       "...             ...       ...       ...       ...       ...       ...   \n",
       "G18996472  0.816817  0.331017  0.983051  0.483149  0.730151  0.374854   \n",
       "119545284  0.492993  0.020586  0.734269  0.574344  0.484472  0.790303   \n",
       "Q57447995  0.939278  0.155785  0.917640  0.112279  0.012447  0.181358   \n",
       "611053577  0.163714  0.793421  0.731699  0.069229  0.217535  0.090502   \n",
       "C13097269  0.641796  0.206206  0.666617  0.142469  0.747868  0.096059   \n",
       "\n",
       "              sCIyG     qaERi  \n",
       "UID                            \n",
       "675919160  0.059355  0.679244  \n",
       "V09461652  0.452404  0.634508  \n",
       "S75396644  0.119394  0.178447  \n",
       "598599835  0.538039  0.164508  \n",
       "W60397022  0.787513  0.029676  \n",
       "...             ...       ...  \n",
       "G18996472  0.311810  0.042603  \n",
       "119545284  0.590182  0.129848  \n",
       "Q57447995  0.104555  0.569813  \n",
       "611053577  0.585255  0.301478  \n",
       "C13097269  0.016536  0.475805  \n",
       "\n",
       "[60000 rows x 92 columns]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2 = outliers_to_median(df2_imputed_dropped)\n",
    "\n",
    "# Display the transformed DataFrame\n",
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array(['Anxiety', 'Death', 'Despair', 'Dread', 'Fatigue', 'Nausea',\n",
       "        'Pain', 'Stress', 'Tears', 'Worthlessness'], dtype=object)]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "category_mappings = enc.categories_\n",
    "category_mappings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1875/1875 [==============================] - 6s 3ms/step\n"
     ]
    }
   ],
   "source": [
    "#predict df2 using model\n",
    "y_pred = model.predict(df2)\n",
    "\n",
    "#use category_mappings\n",
    "y_pred = np.argmax(y_pred, axis=1)\n",
    "\n",
    "y_pred = category_mappings[0][y_pred]\n",
    "y_pred\n",
    "#store only state in csv\n",
    "df2['state']=y_pred\n",
    "df2['state'].to_csv('submission.csv', header=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1875/1875 [==============================] - 3s 2ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\soham\\AppData\\Local\\Temp\\ipykernel_14552\\3806288089.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df2['state'] = y_category\n"
     ]
    }
   ],
   "source": [
    "predictions = []\n",
    "for model in ensemble:\n",
    "    y_pred = model.predict(df2)\n",
    "    predictions.append(y_pred)\n",
    "\n",
    "# Combine predictions using majority voting (mode)\n",
    "ensemble_predictions = pd.DataFrame(predictions).mode(axis=0).squeeze()\n",
    "\n",
    "# Use category_mappings to map ensemble predictions to category names\n",
    "y_category_indices = np.argmax(ensemble_predictions, axis=1)\n",
    "y_category = [category_mappings[i][index] for i, index in enumerate(y_category_indices)]\n",
    "\n",
    "# Assuming UID is in df2 as well\n",
    "uids = df2['UID']\n",
    "\n",
    "# Create a DataFrame with UID and state\n",
    "result_df = pd.DataFrame({'UID': uids, 'state': y_category})\n",
    "\n",
    "# Store the results in a CSV file\n",
    "result_df.to_csv('output.csv', header=True, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = []\n",
    "for model in ensemble:\n",
    "    new_data_predictions = model.predict(df2)\n",
    "    predictions.append(new_data_predictions)\n",
    "\n",
    "# Combine predictions using majority voting (mode)\n",
    "ensemble_predictions = pd.DataFrame(predictions).mode(axis=0).squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from category mappings \n",
    "y_category = category_mappings[0][ensemble_predictions]\n",
    "#store UID and state in csv\n",
    "df2['state'] = y_category\n",
    "df2['state'].to_csv('output.csv', header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hi\n"
     ]
    }
   ],
   "source": [
    "print('hi')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
